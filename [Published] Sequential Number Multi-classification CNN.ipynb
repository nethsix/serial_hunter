{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras as kr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "from functools import reduce\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D, Input, MaxPooling2D, AveragePooling2D\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUMBER_SIZE = 15\n",
    "THRESHOLD_COUNT = 5\n",
    "\n",
    "BATCH_NUMBER_COUNT_50 = 50\n",
    "SINGLE_SAMPLE_SIZE_50 = BATCH_NUMBER_COUNT_50 * NUMBER_SIZE\n",
    "\n",
    "CLASSES = { \"none\": [1,0,0,0], \"sparse\": [0,1,0,0], \"head-heavy\": [0,0,1,0], \"tail-heavy\": [0,0,0,1] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_single_sample_from_array(numbers):\n",
    "  # Treat the list of phone numbers as a single huge list digits\n",
    "  # pretty much like greyscale images, etc.\n",
    "  # We then specify the number boundaries in the conv net\n",
    "  digit_str = ''.join([str(i) for i in numbers])\n",
    "  # Now create an image-like array\n",
    "  return list(digit_str)\n",
    "\n",
    "def create_single_sample_from_dataframe(dataframe_row):\n",
    "  return create_single_sample_from_array(np.array(dataframe_row))\n",
    "\n",
    "def combine_dfs(dfs, outcome_values=None, shuffle=True):# Combine data with sequence with outcome\n",
    "  df_list = []\n",
    "  for idx, df in enumerate(dfs):\n",
    "    data_arr = []\n",
    "    for index, series in df.iterrows():\n",
    "      series_value = [v for _, v in series.iteritems()]\n",
    "      combined_series = create_single_sample_from_array(series_value)\n",
    "      data_arr.append(combined_series)\n",
    "    \n",
    "    df_arr_row_count = len(data_arr)\n",
    "            \n",
    "    outcome_value = outcome_values[idx] if outcome_values else idx\n",
    "    data_outcome_tuples = zip(data_arr, [outcome_value] * df_arr_row_count)\n",
    "\n",
    "    # DEBUG\n",
    "    print(\"outcome_value:%s, outcome_values:%s\" % (outcome_value, outcome_values))\n",
    "\n",
    "    df_list += data_outcome_tuples   \n",
    "    \n",
    "  # Merge data with sequence and no sequence\n",
    "  dataframe = pd.DataFrame(df_list, columns=[\"sample\",\"outcome\"])\n",
    "  # Shuffle (or rather randomly select samples) but 1.0 means all\n",
    "  df_random = dataframe.sample(frac=1) if shuffle else dataframe\n",
    "  return df_random\n",
    "\n",
    "def extract_sample_and_outcome(df, sample_col_name='sample', outcome_col_name='outcome'):\n",
    "  X = [i for i in df[sample_col_name]]\n",
    "  # Y = [int(i) for i in df[outcome_col_name]]\n",
    "  Y = [i for i in df[outcome_col_name]]\n",
    "  return (X, Y)\n",
    "\n",
    "def load_data(filename, expected_shape_tuple):\n",
    "  data = pd.read_csv(filename, header=None)\n",
    "  print(\"data file:%s, data.shape (should be %s): %s\" % (filename, expected_shape_tuple, data.shape))\n",
    "  if data.shape != expected_shape_tuple:\n",
    "    raise ValueError(\"data.shape:%s does not match excpected shape:%s\" % (data.shape, expected_shape_tuple))\n",
    "  return data\n",
    "    \n",
    "def prepare_data(filenames, expected_shape_tuples, outcome_values=None, shuffle=True):\n",
    "  dfs = []\n",
    "  for idx, filename in enumerate(filenames):\n",
    "    dfs.append(load_data(filename, expected_shape_tuples[idx]))\n",
    "\n",
    "  df_all = combine_dfs(dfs, outcome_values, shuffle)\n",
    "  if (df_all.shape[0] != reduce((lambda m, i: m + i.shape[0]), dfs,0)):\n",
    "    raise ValueError(\"There is a problem with combine_seq_no_seq. df_all.shape:%s does not match the sum of df_seq:%s and df_no_seq:%s\" % (df_all.shape, df_seq.shape, df_no_seq.shape))\n",
    "  return df_all\n",
    "\n",
    "def prepare_train_data(dataframe, single_sample_size):\n",
    "  df_row_count = dataframe.shape[0]\n",
    "  (X, Y) = extract_sample_and_outcome(dataframe)\n",
    "  X_train = np.array(X).reshape(df_row_count, 1, single_sample_size, 1)\n",
    "  Y_train = np.array(Y).reshape(df_row_count, len(CLASSES.keys()))\n",
    "\n",
    "  print(\"type(X):%s, len(X):%d, len(X[0]):%d\" % (type(X), len(X), len(X[0])))\n",
    "  print(\"df_row_count: %d\" % df_row_count)\n",
    "  print(\"single_sample_size_bit:%d\" % single_sample_size)\n",
    "  print(\"X_train.shape:%s, Y_train.shape:%s\" % (X_train.shape, Y_train.shape))\n",
    "  print(\"\\n\")\n",
    "\n",
    "  print(\"type(Y[0]):%s, Y[0]:%s\" % (type(Y[0]), Y[0]))\n",
    "\n",
    "  return (X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data file:data_no_sequence_2000_sample_number_50.csv, data.shape (should be (2000, 50)): (2000, 50)\n",
      "data file:data_sequence_sparse_ooo_mid_combo_2000_sample_number_50.csv, data.shape (should be (2000, 50)): (2000, 50)\n",
      "data file:data_sequence_head_heavy_ooo_mid_combo_2000_sample_number_50.csv, data.shape (should be (2000, 50)): (2000, 50)\n",
      "data file:data_sequence_tail_heavy_ooo_mid_combo_2000_sample_number_50.csv, data.shape (should be (2000, 50)): (2000, 50)\n",
      "outcome_value:[1, 0, 0, 0], outcome_values:[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
      "outcome_value:[0, 1, 0, 0], outcome_values:[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
      "outcome_value:[0, 0, 1, 0], outcome_values:[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
      "outcome_value:[0, 0, 0, 1], outcome_values:[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
      "data file:data_no_sequence_10000_sample_number_50.csv, data.shape (should be (10000, 50)): (10000, 50)\n",
      "data file:data_sequence_sparse_ooo_mid_combo_10000_sample_number_50.csv, data.shape (should be (10000, 50)): (10000, 50)\n",
      "data file:data_sequence_head_heavy_ooo_mid_combo_10000_sample_number_50.csv, data.shape (should be (10000, 50)): (10000, 50)\n",
      "data file:data_sequence_tail_heavy_ooo_mid_combo_10000_sample_number_50.csv, data.shape (should be (10000, 50)): (10000, 50)\n",
      "outcome_value:[1, 0, 0, 0], outcome_values:[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
      "outcome_value:[0, 1, 0, 0], outcome_values:[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
      "outcome_value:[0, 0, 1, 0], outcome_values:[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
      "outcome_value:[0, 0, 0, 1], outcome_values:[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Load data from files into dataframe\n",
    "\n",
    "df_random_with_mid_ooo_2000_2000_2000_2000_sample_number_50 = prepare_data(['data_no_sequence_2000_sample_number_50.csv', 'data_sequence_sparse_ooo_mid_combo_2000_sample_number_50.csv', 'data_sequence_head_heavy_ooo_mid_combo_2000_sample_number_50.csv', 'data_sequence_tail_heavy_ooo_mid_combo_2000_sample_number_50.csv'], [(2000,50), (2000,50), (2000,50), (2000,50)], [CLASSES[\"none\"], CLASSES[\"sparse\"], CLASSES[\"head-heavy\"], CLASSES[\"tail-heavy\"]])\n",
    "\n",
    "df_random_with_mid_ooo_10000_10000_10000_10000_sample_number_50 = prepare_data(['data_no_sequence_10000_sample_number_50.csv', 'data_sequence_sparse_ooo_mid_combo_10000_sample_number_50.csv', 'data_sequence_head_heavy_ooo_mid_combo_10000_sample_number_50.csv', 'data_sequence_tail_heavy_ooo_mid_combo_10000_sample_number_50.csv'], [(10000,50), (10000,50), (10000,50), (10000,50)], [CLASSES[\"none\"], CLASSES[\"sparse\"], CLASSES[\"head-heavy\"], CLASSES[\"tail-heavy\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[8, 5, 3, 9, 6, 8, 4, 7, 7, 0, 3, 4, 3, 9, 8, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6812</th>\n",
       "      <td>[4, 6, 4, 0, 3, 3, 0, 3, 8, 1, 8, 2, 0, 2, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5641</th>\n",
       "      <td>[3, 6, 3, 2, 2, 1, 7, 6, 3, 0, 8, 0, 3, 2, 8, ...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7227</th>\n",
       "      <td>[3, 2, 7, 5, 2, 9, 2, 3, 6, 5, 6, 7, 6, 2, 8, ...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2843</th>\n",
       "      <td>[7, 4, 5, 5, 6, 4, 1, 5, 2, 7, 3, 2, 3, 9, 9, ...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>[9, 0, 7, 2, 6, 4, 5, 5, 1, 4, 2, 4, 3, 3, 2, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6709</th>\n",
       "      <td>[1, 4, 5, 7, 6, 1, 1, 4, 1, 4, 5, 7, 4, 9, 4, ...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4621</th>\n",
       "      <td>[6, 7, 3, 4, 9, 0, 1, 7, 7, 5, 4, 0, 9, 4, 8, ...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7357</th>\n",
       "      <td>[7, 9, 5, 4, 4, 2, 9, 6, 6, 9, 7, 0, 2, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6472</th>\n",
       "      <td>[2, 0, 9, 6, 5, 0, 1, 6, 7, 3, 6, 7, 1, 4, 1, ...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7890</th>\n",
       "      <td>[5, 6, 1, 9, 9, 7, 1, 6, 8, 6, 2, 1, 2, 1, 9, ...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5715</th>\n",
       "      <td>[7, 7, 3, 6, 7, 7, 3, 0, 1, 3, 1, 4, 3, 8, 5, ...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5211</th>\n",
       "      <td>[8, 7, 6, 9, 0, 2, 4, 4, 9, 3, 3, 4, 7, 0, 7, ...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>[2, 2, 6, 4, 2, 0, 2, 2, 7, 9, 4, 2, 6, 4, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>[7, 4, 9, 3, 8, 7, 9, 1, 9, 6, 5, 3, 0, 7, 9, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2233</th>\n",
       "      <td>[4, 3, 2, 7, 5, 1, 3, 9, 4, 3, 2, 4, 3, 1, 8, ...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3474</th>\n",
       "      <td>[9, 6, 1, 9, 7, 9, 4, 9, 5, 8, 9, 4, 7, 0, 5, ...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7884</th>\n",
       "      <td>[9, 2, 4, 8, 7, 3, 3, 4, 4, 5, 6, 8, 4, 6, 7, ...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5105</th>\n",
       "      <td>[1, 7, 7, 0, 4, 2, 4, 8, 9, 8, 9, 2, 1, 4, 4, ...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097</th>\n",
       "      <td>[9, 5, 1, 0, 8, 9, 6, 5, 7, 1, 6, 3, 3, 6, 1, ...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sample       outcome\n",
       "27    [8, 5, 3, 9, 6, 8, 4, 7, 7, 0, 3, 4, 3, 9, 8, ...  [1, 0, 0, 0]\n",
       "6812  [4, 6, 4, 0, 3, 3, 0, 3, 8, 1, 8, 2, 0, 2, 0, ...  [0, 0, 0, 1]\n",
       "5641  [3, 6, 3, 2, 2, 1, 7, 6, 3, 0, 8, 0, 3, 2, 8, ...  [0, 0, 1, 0]\n",
       "7227  [3, 2, 7, 5, 2, 9, 2, 3, 6, 5, 6, 7, 6, 2, 8, ...  [0, 0, 0, 1]\n",
       "2843  [7, 4, 5, 5, 6, 4, 1, 5, 2, 7, 3, 2, 3, 9, 9, ...  [0, 1, 0, 0]\n",
       "962   [9, 0, 7, 2, 6, 4, 5, 5, 1, 4, 2, 4, 3, 3, 2, ...  [1, 0, 0, 0]\n",
       "6709  [1, 4, 5, 7, 6, 1, 1, 4, 1, 4, 5, 7, 4, 9, 4, ...  [0, 0, 0, 1]\n",
       "4621  [6, 7, 3, 4, 9, 0, 1, 7, 7, 5, 4, 0, 9, 4, 8, ...  [0, 0, 1, 0]\n",
       "7357  [7, 9, 5, 4, 4, 2, 9, 6, 6, 9, 7, 0, 2, 1, 1, ...  [0, 0, 0, 1]\n",
       "6472  [2, 0, 9, 6, 5, 0, 1, 6, 7, 3, 6, 7, 1, 4, 1, ...  [0, 0, 0, 1]\n",
       "7890  [5, 6, 1, 9, 9, 7, 1, 6, 8, 6, 2, 1, 2, 1, 9, ...  [0, 0, 0, 1]\n",
       "5715  [7, 7, 3, 6, 7, 7, 3, 0, 1, 3, 1, 4, 3, 8, 5, ...  [0, 0, 1, 0]\n",
       "5211  [8, 7, 6, 9, 0, 2, 4, 4, 9, 3, 3, 4, 7, 0, 7, ...  [0, 0, 1, 0]\n",
       "531   [2, 2, 6, 4, 2, 0, 2, 2, 7, 9, 4, 2, 6, 4, 0, ...  [1, 0, 0, 0]\n",
       "1596  [7, 4, 9, 3, 8, 7, 9, 1, 9, 6, 5, 3, 0, 7, 9, ...  [1, 0, 0, 0]\n",
       "2233  [4, 3, 2, 7, 5, 1, 3, 9, 4, 3, 2, 4, 3, 1, 8, ...  [0, 1, 0, 0]\n",
       "3474  [9, 6, 1, 9, 7, 9, 4, 9, 5, 8, 9, 4, 7, 0, 5, ...  [0, 1, 0, 0]\n",
       "7884  [9, 2, 4, 8, 7, 3, 3, 4, 4, 5, 6, 8, 4, 6, 7, ...  [0, 0, 0, 1]\n",
       "5105  [1, 7, 7, 0, 4, 2, 4, 8, 9, 8, 9, 2, 1, 4, 4, ...  [0, 0, 1, 0]\n",
       "3097  [9, 5, 1, 0, 8, 9, 6, 5, 7, 1, 6, 3, 3, 6, 1, ...  [0, 1, 0, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #1\n",
    "\n",
    "# There should be 'sample' and 'outcome' columns\n",
    "# 'sample' contains all phone numbers in period broken down into individual digits\n",
    "# and combined\n",
    "# 'outcome' is 0 or 1 for binary\n",
    "# Also there should be mixed 0 and 1 since the samples have been randomize\n",
    "\n",
    "df_random_with_mid_ooo_2000_2000_2000_2000_sample_number_50.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #2\n",
    "\n",
    "# 25% or 2000 samples should be 0,0,0,1\n",
    "np.count_nonzero(df_random_with_mid_ooo_2000_2000_2000_2000_sample_number_50['outcome'].apply(lambda x: x == [0,0,0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "750"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #3\n",
    "\n",
    "# The 'sample' length should be same as SINGLE_SAMPLE_SIZE_50 = BATCH_NUMBER_COUNT_50 * NUMBER_SIZE\n",
    "len(df_random_with_mid_ooo_2000_2000_2000_2000_sample_number_50['sample'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(X):<class 'list'>, len(X):8000, len(X[0]):750\n",
      "df_row_count: 8000\n",
      "single_sample_size_bit:750\n",
      "X_train.shape:(8000, 1, 750, 1), Y_train.shape:(8000, 4)\n",
      "\n",
      "\n",
      "type(Y[0]):<class 'list'>, Y[0]:[1, 0, 0, 0]\n",
      "type(X):<class 'list'>, len(X):40000, len(X[0]):750\n",
      "df_row_count: 40000\n",
      "single_sample_size_bit:750\n",
      "X_train.shape:(40000, 1, 750, 1), Y_train.shape:(40000, 4)\n",
      "\n",
      "\n",
      "type(Y[0]):<class 'list'>, Y[0]:[1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Split out dataframe containing both samples and outcome\n",
    "\n",
    "(X_2000_2000_2000_2000_with_mid_ooo_sample_number_50_train, Y_2000_2000_2000_2000_with_mid_ooo_sample_number_50_train) = prepare_train_data(df_random_with_mid_ooo_2000_2000_2000_2000_sample_number_50, SINGLE_SAMPLE_SIZE_50)\n",
    "\n",
    "(X_10000_10000_10000_10000_with_mid_ooo_sample_number_50_train, Y_10000_10000_10000_10000_with_mid_ooo_sample_number_50_train) = prepare_train_data(df_random_with_mid_ooo_10000_10000_10000_10000_sample_number_50, SINGLE_SAMPLE_SIZE_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 1, 750, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For CNN, the dimensions are number of samples, height, width, channel/feature maps\n",
    "# Number of samples: sum of 'attack' and 'no attack' samples\n",
    "# Height: 1\n",
    "# Width: SINGLE_SAMPLE_SIZE_50\n",
    "# Feature maps/Channels: 1\n",
    "X_2000_2000_2000_2000_with_mid_ooo_sample_number_50_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #5\n",
    "\n",
    "# The total rows should be attack and non-attack rows\n",
    "# The wide should be array of 4, since it can be [0,0,0,0], [0,0,0,1], etc.\n",
    "Y_2000_2000_2000_2000_with_mid_ooo_sample_number_50_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Various multi-class CNN models tested\n",
    "\n",
    "def create_cnn_model_G_50():\n",
    "  # Create model\n",
    "  model = Sequential()\n",
    "  # Max number of digits are different in number yet still can be within threshold\n",
    "  digit_diff_size = 2\n",
    "  digit_same_size = NUMBER_SIZE - digit_diff_size\n",
    "  # If the seq is in the middle then parts that are same is split in two\n",
    "  smallest_digit_same_size = math.floor(digit_same_size/2)\n",
    "  # add model layers\n",
    "  number_digit_diff_size = math.floor(NUMBER_SIZE/digit_diff_size)\n",
    "  model.add(Conv2D(50, kernel_size=(1,smallest_digit_same_size), strides=(1,1), activation='relu', input_shape=(1,SINGLE_SAMPLE_SIZE_50,1)))\n",
    "  model.add(Conv2D(20, kernel_size=(1,NUMBER_SIZE), strides=(1,NUMBER_SIZE), activation='relu'))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(BATCH_NUMBER_COUNT_50, activation=\"relu\"))\n",
    "  model.add(Dense(4, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_cnn_model_G_50_coarse():\n",
    "  # Create model\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(50, kernel_size=(1,NUMBER_SIZE), strides=(1,NUMBER_SIZE), activation='relu', input_shape=(1,SINGLE_SAMPLE_SIZE_50,1)))\n",
    "  model.add(Conv2D(20, kernel_size=(1,1), strides=(1,1), activation='relu'))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(BATCH_NUMBER_COUNT_50, activation=\"relu\"))\n",
    "  model.add(Dense(4, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_cnn_model_G_50_coarse_2():\n",
    "  # Create model\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(40, kernel_size=(1,NUMBER_SIZE), strides=(1,NUMBER_SIZE), activation='relu', input_shape=(1,SINGLE_SAMPLE_SIZE_50,1)))\n",
    "  model.add(MaxPooling2D(pool_size=(1,1), strides=(1,1)))\n",
    "  model.add(Conv2D(20, kernel_size=(1,1), strides=(1,1), activation='relu'))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(BATCH_NUMBER_COUNT_50, activation=\"relu\"))\n",
    "  model.add(Dense(4, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_cnn_model_G_50_coarse_3():\n",
    "  # Create model\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(50, kernel_size=(1,NUMBER_SIZE), strides=(1,NUMBER_SIZE), activation='relu', input_shape=(1,SINGLE_SAMPLE_SIZE_50,1)))\n",
    "  # MaxPooling2D 1x1 theoretically does nothing but without it the trained model is 5% less accurate\n",
    "  model.add(MaxPooling2D(pool_size=(1,1), strides=(1,1)))\n",
    "  model.add(Conv2D(30, kernel_size=(1,1), strides=(1,1), activation='relu'))\n",
    "  # MaxPooling2D 1x1 theoretically does nothing but without it the trained model is 5% less accurate\n",
    "  model.add(MaxPooling2D(pool_size=(1,1), strides=(1,1)))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(BATCH_NUMBER_COUNT_50, activation=\"relu\"))\n",
    "  model.add(Dense(4, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_cnn_model_G_50_coarse_4():\n",
    "  # Create model\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(50, kernel_size=(1,NUMBER_SIZE), strides=(1,NUMBER_SIZE), activation='relu', input_shape=(1,SINGLE_SAMPLE_SIZE_50,1)))\n",
    "  model.add(MaxPooling2D(pool_size=(1,1), strides=(1,1)))\n",
    "  model.add(Conv2D(30, kernel_size=(1,1), strides=(1,1), activation='relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(1,1), strides=(1,1)))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(int(BATCH_NUMBER_COUNT_50), activation=\"relu\"))\n",
    "  model.add(Dense(4, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_cnn_model_G_50_mod():\n",
    "  # Create model\n",
    "  model = Sequential()\n",
    "  # Max number of bits are different in number yet still can be within threshold\n",
    "  bit_diff_size = BIT_PER_DIGIT*2\n",
    "  bit_same_size = NUMBER_SIZE_BIT - bit_diff_size\n",
    "  # If the seq is in the middle then parts that are same is split in two\n",
    "  smallest_bit_same_size = int(math.floor(bit_same_size/2) / BIT_PER_DIGIT)\n",
    "  # add model layers\n",
    "  number_bit_diff_size = math.floor(NUMBER_SIZE_BIT/bit_diff_size)\n",
    "  model.add(Conv2D(150, kernel_size=(1,smallest_bit_same_size), strides=(1,BIT_PER_DIGIT), activation='relu', input_shape=(1,SINGLE_SAMPLE_SIZE_BIT_50,1)))\n",
    "  model.add(Conv2D(5, kernel_size=(1,NUMBER_SIZE_BIT ), strides=(1,NUMBER_SIZE_BIT), activation='relu'))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(10 - THRESHOLD_COUNT, activation=\"relu\"))\n",
    "  model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_cnn_model_G_50_maxpool():\n",
    "  # Create model\n",
    "  model = Sequential()\n",
    "  # Max number of bits are different in number yet still can be within threshold\n",
    "  digit_diff_size = 2\n",
    "  digit_same_size = NUMBER_SIZE - digit_diff_size\n",
    "  # If the seq is in the middle then parts that are same is split in two\n",
    "  smallest_digit_same_size = math.floor(digit_same_size/2)\n",
    "  # add model layers\n",
    "  number_digit_diff_size = math.floor(NUMBER_SIZE/digit_diff_size)\n",
    "  model.add(Conv2D(50, kernel_size=(1,smallest_digit_same_size), strides=(1,1), activation='relu', input_shape=(1,SINGLE_SAMPLE_SIZE_50,1)))\n",
    "  model.add(Conv2D(20, kernel_size=(1,NUMBER_SIZE), strides=(1,NUMBER_SIZE), activation='relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(1,1), strides=(1,1)))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(BATCH_NUMBER_COUNT_50, activation=\"relu\"))\n",
    "  model.add(Dense(4, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_cnn_model_G_50_avepool():\n",
    "  # Create model\n",
    "  model = Sequential()\n",
    "  # Max number of bits are different in number yet still can be within threshold\n",
    "  digit_diff_size = 2\n",
    "  digit_same_size = NUMBER_SIZE - digit_diff_size\n",
    "  # If the seq is in the middle then parts that are same is split in two\n",
    "  smallest_digit_same_size = math.floor(digit_same_size/2)\n",
    "  # add model layers\n",
    "  number_digit_diff_size = math.floor(NUMBER_SIZE/digit_diff_size)\n",
    "  model.add(Conv2D(50, kernel_size=(1,smallest_digit_same_size), strides=(1,1), activation='relu', input_shape=(1,SINGLE_SAMPLE_SIZE_50,1)))\n",
    "  model.add(AveragePooling2D(pool_size=(1,NUMBER_SIZE), strides=(1,NUMBER_SIZE)))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(BATCH_NUMBER_COUNT_50, activation=\"relu\"))\n",
    "  model.add(Dense(4, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/15\n",
      "32000/32000 [==============================] - 23s - loss: 1.2861 - acc: 0.3359 - val_loss: 0.9395 - val_acc: 0.5761\n",
      "Epoch 2/15\n",
      "32000/32000 [==============================] - 22s - loss: 0.8790 - acc: 0.6025 - val_loss: 0.8157 - val_acc: 0.6388\n",
      "Epoch 3/15\n",
      "32000/32000 [==============================] - 23s - loss: 0.8056 - acc: 0.6356 - val_loss: 0.7836 - val_acc: 0.6404\n",
      "Epoch 4/15\n",
      "32000/32000 [==============================] - 23s - loss: 0.7752 - acc: 0.6441 - val_loss: 0.7912 - val_acc: 0.6348\n",
      "Epoch 5/15\n",
      "32000/32000 [==============================] - 26s - loss: 0.7557 - acc: 0.6514 - val_loss: 0.7497 - val_acc: 0.6566\n",
      "Epoch 6/15\n",
      "32000/32000 [==============================] - 26s - loss: 0.7449 - acc: 0.6565 - val_loss: 0.7767 - val_acc: 0.6415\n",
      "Epoch 7/15\n",
      "32000/32000 [==============================] - 23s - loss: 0.7337 - acc: 0.6635 - val_loss: 0.7314 - val_acc: 0.6646\n",
      "Epoch 8/15\n",
      "32000/32000 [==============================] - 23s - loss: 0.7237 - acc: 0.6660 - val_loss: 0.7511 - val_acc: 0.6526\n",
      "Epoch 9/15\n",
      "32000/32000 [==============================] - 23s - loss: 0.7101 - acc: 0.6735 - val_loss: 0.7388 - val_acc: 0.6636\n",
      "Epoch 10/15\n",
      "32000/32000 [==============================] - 24s - loss: 0.7021 - acc: 0.6786 - val_loss: 0.7308 - val_acc: 0.6693\n",
      "Epoch 11/15\n",
      "32000/32000 [==============================] - 23s - loss: 0.6886 - acc: 0.6856 - val_loss: 0.7237 - val_acc: 0.6755\n",
      "Epoch 12/15\n",
      "32000/32000 [==============================] - 23s - loss: 0.6758 - acc: 0.6930 - val_loss: 0.7159 - val_acc: 0.6783\n",
      "Epoch 13/15\n",
      "32000/32000 [==============================] - 23s - loss: 0.6556 - acc: 0.7058 - val_loss: 0.7054 - val_acc: 0.6795\n",
      "Epoch 14/15\n",
      "32000/32000 [==============================] - 23s - loss: 0.6419 - acc: 0.7148 - val_loss: 0.7025 - val_acc: 0.6851\n",
      "Epoch 15/15\n",
      "32000/32000 [==============================] - 23s - loss: 0.6333 - acc: 0.7180 - val_loss: 0.7047 - val_acc: 0.6920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1374d05f8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The best is perhaps: create_cnn_model_G_50_coarse_3\n",
    "\n",
    "cnn_model_G_50_coarse_3 = create_cnn_model_G_50_coarse_3()\n",
    "\n",
    "cnn_model_G_50_coarse_3.fit(X_10000_10000_10000_10000_with_mid_ooo_sample_number_50_train, Y_10000_10000_10000_10000_with_mid_ooo_sample_number_50_train, validation_split=0.2, batch_size=10, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7872/8000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.679"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_arr = cnn_model_G_50_coarse_3.predict_classes(X_2000_2000_2000_2000_with_mid_ooo_sample_number_50_train)\n",
    "# Each element in 'a' will contain 1/True if prediction matches expected outcome\n",
    "expected_arr = [np.where(r==1)[0][0] for r in Y_2000_2000_2000_2000_with_mid_ooo_sample_number_50_train]\n",
    "a = (predict_arr==expected_arr)\n",
    "# % of correct predictions\n",
    "np.count_nonzero(a)/np.size(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: We cannot use StratifiedKFold for multi-class unless we do some transformation\n",
    "# https://stackoverflow.com/questions/48508036/sklearn-stratifiedkfold-valueerror-supported-target-types-are-binary-mul\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "seed = 7\n",
    "estimator = KerasClassifier(build_fn=create_cnn_model_G_50_coarse_3, epochs=20, batch_size=10, verbose=1)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X_10000_10000_10000_10000_with_mid_ooo_sample_number_50_train, Y_10000_10000_10000_10000_with_mid_ooo_sample_number_50_train, cv=kfold)\n",
    "\n",
    "print(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: This is ONLY for saving models\n",
    "model_json = cnn_model_G_50_coarse_3.to_json()\n",
    "with open(\"cnn_model_G_50_coarse-multi_class_non_binary_sparse-tail_heavy-head_heavy_train_mid_ooo_10000-sample_71-pct-20190518.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "cnn_model_G_50_coarse_3.save('cnn_model_G_50_coarse-multi_class_non_binary_sparse-tail_heavy-head_heavy_train_mid_ooo_10000-sample_71-pct-20190518.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Additional measurements to ensure that the accuracy is not skewed by being able to\n",
    "# detect attacks very well of which there are 3 classes (75%)\n",
    "# but not detect non-attacks which is 1 class (25%)\n",
    "# So we measure accurary with data set where 50% is non attack, and 16.67% for\n",
    "# each of the 3 attack classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "cnn_model_G_50_coarse_3 = create_cnn_model_G_50_coarse_3()\n",
    "cnn_model_G_50_coarse_3.load_weights('cnn_model_G_50_coarse-multi_class_non_binary_sparse-tail_heavy-head_heavy_train_mid_ooo_10000-sample_71-pct-20190518.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data file:data_no_sequence_6000_sample_number_50.csv, data.shape (should be (6000, 50)): (6000, 50)\n",
      "data file:data_sequence_sparse_ooo_mid_combo_2000_sample_number_50.csv, data.shape (should be (2000, 50)): (2000, 50)\n",
      "data file:data_sequence_head_heavy_ooo_mid_combo_2000_sample_number_50.csv, data.shape (should be (2000, 50)): (2000, 50)\n",
      "data file:data_sequence_tail_heavy_ooo_mid_combo_2000_sample_number_50.csv, data.shape (should be (2000, 50)): (2000, 50)\n",
      "outcome_value:[1, 0, 0, 0], outcome_values:[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
      "outcome_value:[0, 1, 0, 0], outcome_values:[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
      "outcome_value:[0, 0, 1, 0], outcome_values:[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
      "outcome_value:[0, 0, 0, 1], outcome_values:[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "df_random_with_mid_ooo_6000_2000_2000_2000_sample_number_50 = prepare_data(['data_no_sequence_6000_sample_number_50.csv', 'data_sequence_sparse_ooo_mid_combo_2000_sample_number_50.csv', 'data_sequence_head_heavy_ooo_mid_combo_2000_sample_number_50.csv', 'data_sequence_tail_heavy_ooo_mid_combo_2000_sample_number_50.csv'], [(6000,50), (2000,50), (2000,50), (2000,50)], [CLASSES[\"none\"], CLASSES[\"sparse\"], CLASSES[\"head-heavy\"], CLASSES[\"tail-heavy\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10306</th>\n",
       "      <td>[9, 4, 6, 8, 2, 6, 2, 5, 7, 7, 5, 9, 0, 9, 6, ...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10143</th>\n",
       "      <td>[7, 3, 8, 5, 7, 1, 5, 3, 4, 7, 6, 0, 1, 2, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>[1, 3, 6, 1, 0, 3, 4, 7, 0, 8, 3, 8, 7, 8, 1, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11756</th>\n",
       "      <td>[9, 1, 8, 7, 1, 6, 2, 0, 3, 5, 5, 5, 1, 5, 6, ...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7343</th>\n",
       "      <td>[7, 3, 9, 7, 9, 4, 0, 0, 0, 9, 0, 1, 8, 5, 2, ...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>[7, 2, 5, 7, 9, 3, 9, 7, 0, 9, 9, 1, 6, 2, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>[3, 0, 4, 2, 4, 7, 9, 8, 9, 6, 6, 4, 5, 6, 7, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>[3, 5, 9, 0, 0, 5, 4, 6, 3, 2, 5, 6, 5, 8, 7, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2881</th>\n",
       "      <td>[5, 1, 5, 1, 9, 6, 4, 1, 9, 2, 1, 1, 4, 5, 7, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>[7, 0, 3, 3, 4, 2, 6, 0, 6, 6, 0, 9, 7, 1, 7, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7724</th>\n",
       "      <td>[2, 6, 9, 1, 0, 9, 9, 3, 5, 5, 8, 8, 5, 4, 7, ...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5639</th>\n",
       "      <td>[6, 5, 9, 7, 5, 0, 8, 1, 3, 9, 3, 6, 8, 9, 7, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3951</th>\n",
       "      <td>[7, 2, 6, 2, 6, 5, 9, 9, 2, 4, 2, 0, 4, 3, 5, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11812</th>\n",
       "      <td>[1, 9, 0, 8, 1, 6, 3, 2, 8, 3, 0, 5, 9, 3, 1, ...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8420</th>\n",
       "      <td>[1, 8, 1, 2, 6, 8, 6, 1, 9, 3, 1, 1, 6, 7, 2, ...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7054</th>\n",
       "      <td>[3, 2, 7, 7, 6, 4, 3, 3, 1, 6, 7, 0, 9, 1, 2, ...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5669</th>\n",
       "      <td>[7, 9, 6, 7, 6, 3, 7, 7, 0, 7, 9, 8, 6, 5, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10891</th>\n",
       "      <td>[9, 8, 4, 1, 3, 4, 5, 2, 3, 6, 0, 1, 0, 7, 6, ...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11816</th>\n",
       "      <td>[9, 0, 4, 2, 2, 0, 8, 9, 4, 2, 3, 6, 7, 6, 9, ...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6010</th>\n",
       "      <td>[5, 7, 3, 1, 0, 0, 4, 8, 8, 9, 0, 7, 8, 7, 4, ...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  sample       outcome\n",
       "10306  [9, 4, 6, 8, 2, 6, 2, 5, 7, 7, 5, 9, 0, 9, 6, ...  [0, 0, 0, 1]\n",
       "10143  [7, 3, 8, 5, 7, 1, 5, 3, 4, 7, 6, 0, 1, 2, 0, ...  [0, 0, 0, 1]\n",
       "5207   [1, 3, 6, 1, 0, 3, 4, 7, 0, 8, 3, 8, 7, 8, 1, ...  [1, 0, 0, 0]\n",
       "11756  [9, 1, 8, 7, 1, 6, 2, 0, 3, 5, 5, 5, 1, 5, 6, ...  [0, 0, 0, 1]\n",
       "7343   [7, 3, 9, 7, 9, 4, 0, 0, 0, 9, 0, 1, 8, 5, 2, ...  [0, 1, 0, 0]\n",
       "1471   [7, 2, 5, 7, 9, 3, 9, 7, 0, 9, 9, 1, 6, 2, 0, ...  [1, 0, 0, 0]\n",
       "1663   [3, 0, 4, 2, 4, 7, 9, 8, 9, 6, 6, 4, 5, 6, 7, ...  [1, 0, 0, 0]\n",
       "2000   [3, 5, 9, 0, 0, 5, 4, 6, 3, 2, 5, 6, 5, 8, 7, ...  [1, 0, 0, 0]\n",
       "2881   [5, 1, 5, 1, 9, 6, 4, 1, 9, 2, 1, 1, 4, 5, 7, ...  [1, 0, 0, 0]\n",
       "425    [7, 0, 3, 3, 4, 2, 6, 0, 6, 6, 0, 9, 7, 1, 7, ...  [1, 0, 0, 0]\n",
       "7724   [2, 6, 9, 1, 0, 9, 9, 3, 5, 5, 8, 8, 5, 4, 7, ...  [0, 1, 0, 0]\n",
       "5639   [6, 5, 9, 7, 5, 0, 8, 1, 3, 9, 3, 6, 8, 9, 7, ...  [1, 0, 0, 0]\n",
       "3951   [7, 2, 6, 2, 6, 5, 9, 9, 2, 4, 2, 0, 4, 3, 5, ...  [1, 0, 0, 0]\n",
       "11812  [1, 9, 0, 8, 1, 6, 3, 2, 8, 3, 0, 5, 9, 3, 1, ...  [0, 0, 0, 1]\n",
       "8420   [1, 8, 1, 2, 6, 8, 6, 1, 9, 3, 1, 1, 6, 7, 2, ...  [0, 0, 1, 0]\n",
       "7054   [3, 2, 7, 7, 6, 4, 3, 3, 1, 6, 7, 0, 9, 1, 2, ...  [0, 1, 0, 0]\n",
       "5669   [7, 9, 6, 7, 6, 3, 7, 7, 0, 7, 9, 8, 6, 5, 0, ...  [1, 0, 0, 0]\n",
       "10891  [9, 8, 4, 1, 3, 4, 5, 2, 3, 6, 0, 1, 0, 7, 6, ...  [0, 0, 0, 1]\n",
       "11816  [9, 0, 4, 2, 2, 0, 8, 9, 4, 2, 3, 6, 7, 6, 9, ...  [0, 0, 0, 1]\n",
       "6010   [5, 7, 3, 1, 0, 0, 4, 8, 8, 9, 0, 7, 8, 7, 4, ...  [0, 1, 0, 0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #1\n",
    "\n",
    "# There should be 'sample' and 'outcome' columns\n",
    "# 'sample' contains all phone numbers in period broken down into individual digits\n",
    "# and combined\n",
    "# 'outcome' is 0 or 1 for binary\n",
    "# Also there should be mixed 0 and 1 since the samples have been randomize\n",
    "\n",
    "df_random_with_mid_ooo_6000_2000_2000_2000_sample_number_50.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #2a\n",
    "\n",
    "# 50% or 6000 samples should be 1,0,0,0\n",
    "np.count_nonzero(df_random_with_mid_ooo_6000_2000_2000_2000_sample_number_50['outcome'].apply(lambda x: x == [1,0,0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #2b\n",
    "\n",
    "# 2000 samples should be 0,0,0,1, i.e., other classes\n",
    "np.count_nonzero(df_random_with_mid_ooo_6000_2000_2000_2000_sample_number_50['outcome'].apply(lambda x: x == [0,0,0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "750"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #3\n",
    "\n",
    "# The 'sample' length should be same as SINGLE_SAMPLE_SIZE_50 = BATCH_NUMBER_COUNT_50 * NUMBER_SIZE\n",
    "len(df_random_with_mid_ooo_6000_2000_2000_2000_sample_number_50['sample'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(X):<class 'list'>, len(X):12000, len(X[0]):750\n",
      "df_row_count: 12000\n",
      "single_sample_size_bit:750\n",
      "X_train.shape:(12000, 1, 750, 1), Y_train.shape:(12000, 4)\n",
      "\n",
      "\n",
      "type(Y[0]):<class 'list'>, Y[0]:[0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "(X_6000_2000_2000_2000_with_mid_ooo_sample_number_50_train, Y_6000_2000_2000_2000_with_mid_ooo_sample_number_50_train) = prepare_train_data(df_random_with_mid_ooo_6000_2000_2000_2000_sample_number_50, SINGLE_SAMPLE_SIZE_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 1, 750, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For CNN, the dimensions are number of samples, height, width, channel/feature maps\n",
    "# Number of samples: sum of 'attack' and 'no attack' samples\n",
    "# Height: 1\n",
    "# Width: SINGLE_SAMPLE_SIZE_50\n",
    "# Feature maps/Channels: 1\n",
    "X_6000_2000_2000_2000_with_mid_ooo_sample_number_50_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #5\n",
    "\n",
    "# The total rows should be attack and non-attack rows\n",
    "# The wide should be array of 4, since it can be [0,0,0,0], [0,0,0,1], etc.\n",
    "Y_6000_2000_2000_2000_with_mid_ooo_sample_number_50_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11904/12000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6503333333333333"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_arr = cnn_model_G_50_coarse_3.predict_classes(X_6000_2000_2000_2000_with_mid_ooo_sample_number_50_train)\n",
    "# Each element in 'a' will contain 1/True if prediction matches expected outcome\n",
    "expected_arr = [np.where(r==1)[0][0] for r in Y_6000_2000_2000_2000_with_mid_ooo_sample_number_50_train]\n",
    "a = (predict_arr==expected_arr)\n",
    "# % of correct predictions\n",
    "np.count_nonzero(a)/np.size(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
