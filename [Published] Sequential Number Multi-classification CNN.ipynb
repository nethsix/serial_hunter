{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras as kr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "from functools import reduce\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D, Input, MaxPooling2D, AveragePooling2D\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUMBER_SIZE = 15\n",
    "THRESHOLD_COUNT = 5\n",
    "\n",
    "BATCH_NUMBER_COUNT_50 = 50\n",
    "SINGLE_SAMPLE_SIZE_50 = BATCH_NUMBER_COUNT_50 * NUMBER_SIZE\n",
    "\n",
    "CLASSES = { \"none\": [1,0,0,0], \"sparse\": [0,1,0,0], \"head-heavy\": [0,0,1,0], \"tail-heavy\": [0,0,0,1] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_single_sample_from_array(numbers):\n",
    "  # Treat the list of phone numbers as a single huge list digits\n",
    "  # pretty much like greyscale images, etc.\n",
    "  # We then specify the number boundaries in the conv net\n",
    "  digit_str = ''.join([str(i) for i in numbers])\n",
    "  # Now create an image-like array\n",
    "  return list(digit_str)\n",
    "\n",
    "def create_single_sample_from_dataframe(dataframe_row):\n",
    "  return create_single_sample_from_array(np.array(dataframe_row))\n",
    "\n",
    "def combine_dfs(dfs, outcome_values=None, shuffle=True):# Combine data with sequence with outcome\n",
    "  df_list = []\n",
    "  for idx, df in enumerate(dfs):\n",
    "    data_arr = []\n",
    "    for index, series in df.iterrows():\n",
    "      series_value = [v for _, v in series.iteritems()]\n",
    "      combined_series = create_single_sample_from_array(series_value)\n",
    "      data_arr.append(combined_series)\n",
    "    \n",
    "    df_arr_row_count = len(data_arr)\n",
    "            \n",
    "    outcome_value = outcome_values[idx] if outcome_values else idx\n",
    "    data_outcome_tuples = zip(data_arr, [outcome_value] * df_arr_row_count)\n",
    "\n",
    "    # DEBUG\n",
    "    print(\"outcome_value:%s, outcome_values:%s\" % (outcome_value, outcome_values))\n",
    "\n",
    "    df_list += data_outcome_tuples   \n",
    "    \n",
    "  # Merge data with sequence and no sequence\n",
    "  dataframe = pd.DataFrame(df_list, columns=[\"sample\",\"outcome\"])\n",
    "  # Shuffle (or rather randomly select samples) but 1.0 means all\n",
    "  df_random = dataframe.sample(frac=1) if shuffle else dataframe\n",
    "  return df_random\n",
    "\n",
    "def extract_sample_and_outcome(df, sample_col_name='sample', outcome_col_name='outcome'):\n",
    "  X = [i for i in df[sample_col_name]]\n",
    "  # Y = [int(i) for i in df[outcome_col_name]]\n",
    "  Y = [i for i in df[outcome_col_name]]\n",
    "  return (X, Y)\n",
    "\n",
    "def load_data(filename, expected_shape_tuple):\n",
    "  data = pd.read_csv(filename, header=None)\n",
    "  print(\"data file:%s, data.shape (should be %s): %s\" % (filename, expected_shape_tuple, data.shape))\n",
    "  if data.shape != expected_shape_tuple:\n",
    "    raise ValueError(\"data.shape:%s does not match excpected shape:%s\" % (data.shape, expected_shape_tuple))\n",
    "  return data\n",
    "    \n",
    "def prepare_data(filenames, expected_shape_tuples, outcome_values=None, shuffle=True):\n",
    "  dfs = []\n",
    "  for idx, filename in enumerate(filenames):\n",
    "    dfs.append(load_data(filename, expected_shape_tuples[idx]))\n",
    "\n",
    "  df_all = combine_dfs(dfs, outcome_values, shuffle)\n",
    "  if (df_all.shape[0] != reduce((lambda m, i: m + i.shape[0]), dfs,0)):\n",
    "    raise ValueError(\"There is a problem with combine_seq_no_seq. df_all.shape:%s does not match the sum of df_seq:%s and df_no_seq:%s\" % (df_all.shape, df_seq.shape, df_no_seq.shape))\n",
    "  return df_all\n",
    "\n",
    "def prepare_train_data(dataframe, single_sample_size):\n",
    "  df_row_count = dataframe.shape[0]\n",
    "  (X, Y) = extract_sample_and_outcome(dataframe)\n",
    "  X_train = np.array(X).reshape(df_row_count, 1, single_sample_size, 1)\n",
    "  Y_train = np.array(Y).reshape(df_row_count, len(CLASSES.keys()))\n",
    "\n",
    "  print(\"type(X):%s, len(X):%d, len(X[0]):%d\" % (type(X), len(X), len(X[0])))\n",
    "  print(\"df_row_count: %d\" % df_row_count)\n",
    "  print(\"single_sample_size_bit:%d\" % single_sample_size)\n",
    "  print(\"X_train.shape:%s, Y_train.shape:%s\" % (X_train.shape, Y_train.shape))\n",
    "  print(\"\\n\")\n",
    "\n",
    "  print(\"type(Y[0]):%s, Y[0]:%s\" % (type(Y[0]), Y[0]))\n",
    "\n",
    "  return (X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data file:data_no_sequence_2000_sample_number_50.csv, data.shape (should be (2000, 50)): (2000, 50)\n",
      "data file:data_sequence_sparse_ooo_mid_combo_2000_sample_number_50.csv, data.shape (should be (2000, 50)): (2000, 50)\n",
      "data file:data_sequence_head_heavy_ooo_mid_combo_2000_sample_number_50.csv, data.shape (should be (2000, 50)): (2000, 50)\n",
      "data file:data_sequence_tail_heavy_ooo_mid_combo_2000_sample_number_50.csv, data.shape (should be (2000, 50)): (2000, 50)\n",
      "outcome_value:[1, 0, 0, 0], outcome_values:[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
      "outcome_value:[0, 1, 0, 0], outcome_values:[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
      "outcome_value:[0, 0, 1, 0], outcome_values:[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
      "outcome_value:[0, 0, 0, 1], outcome_values:[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
      "data file:data_no_sequence_10000_sample_number_50.csv, data.shape (should be (10000, 50)): (10000, 50)\n",
      "data file:data_sequence_sparse_ooo_mid_combo_10000_sample_number_50.csv, data.shape (should be (10000, 50)): (10000, 50)\n",
      "data file:data_sequence_head_heavy_ooo_mid_combo_10000_sample_number_50.csv, data.shape (should be (10000, 50)): (10000, 50)\n",
      "data file:data_sequence_tail_heavy_ooo_mid_combo_10000_sample_number_50.csv, data.shape (should be (10000, 50)): (10000, 50)\n",
      "outcome_value:[1, 0, 0, 0], outcome_values:[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
      "outcome_value:[0, 1, 0, 0], outcome_values:[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
      "outcome_value:[0, 0, 1, 0], outcome_values:[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
      "outcome_value:[0, 0, 0, 1], outcome_values:[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Load data from files into dataframe\n",
    "\n",
    "df_random_with_mid_ooo_2000_2000_2000_2000_sample_number_50 = prepare_data(['data_no_sequence_2000_sample_number_50.csv', 'data_sequence_sparse_ooo_mid_combo_2000_sample_number_50.csv', 'data_sequence_head_heavy_ooo_mid_combo_2000_sample_number_50.csv', 'data_sequence_tail_heavy_ooo_mid_combo_2000_sample_number_50.csv'], [(2000,50), (2000,50), (2000,50), (2000,50)], [CLASSES[\"none\"], CLASSES[\"sparse\"], CLASSES[\"head-heavy\"], CLASSES[\"tail-heavy\"]])\n",
    "\n",
    "df_random_with_mid_ooo_10000_10000_10000_10000_sample_number_50 = prepare_data(['data_no_sequence_10000_sample_number_50.csv', 'data_sequence_sparse_ooo_mid_combo_10000_sample_number_50.csv', 'data_sequence_head_heavy_ooo_mid_combo_10000_sample_number_50.csv', 'data_sequence_tail_heavy_ooo_mid_combo_10000_sample_number_50.csv'], [(10000,50), (10000,50), (10000,50), (10000,50)], [CLASSES[\"none\"], CLASSES[\"sparse\"], CLASSES[\"head-heavy\"], CLASSES[\"tail-heavy\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4812</th>\n",
       "      <td>[8, 0, 3, 6, 0, 6, 3, 7, 1, 9, 9, 7, 9, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2635</th>\n",
       "      <td>[6, 3, 5, 3, 3, 2, 4, 6, 3, 0, 1, 8, 4, 4, 6, ...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5445</th>\n",
       "      <td>[9, 2, 8, 1, 8, 8, 8, 7, 6, 2, 5, 3, 2, 0, 8, ...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5948</th>\n",
       "      <td>[2, 3, 8, 2, 7, 1, 7, 7, 1, 2, 8, 1, 0, 6, 4, ...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6061</th>\n",
       "      <td>[2, 8, 9, 8, 2, 8, 7, 7, 4, 2, 9, 7, 3, 3, 2, ...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3028</th>\n",
       "      <td>[6, 1, 5, 2, 7, 2, 0, 9, 7, 8, 0, 3, 2, 3, 2, ...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6503</th>\n",
       "      <td>[2, 6, 0, 5, 8, 3, 0, 0, 6, 8, 9, 5, 8, 5, 2, ...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354</th>\n",
       "      <td>[1, 1, 7, 8, 2, 9, 1, 0, 9, 7, 3, 9, 8, 3, 2, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3823</th>\n",
       "      <td>[1, 8, 7, 0, 7, 7, 5, 4, 3, 3, 6, 8, 5, 7, 5, ...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4428</th>\n",
       "      <td>[2, 2, 5, 2, 8, 3, 7, 2, 2, 2, 8, 0, 3, 9, 5, ...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>[2, 3, 4, 6, 5, 3, 2, 3, 6, 0, 1, 4, 3, 0, 4, ...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7378</th>\n",
       "      <td>[1, 3, 8, 4, 1, 5, 2, 3, 2, 8, 8, 9, 1, 5, 6, ...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6854</th>\n",
       "      <td>[6, 8, 5, 2, 1, 4, 7, 4, 2, 6, 3, 2, 0, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4221</th>\n",
       "      <td>[9, 3, 3, 5, 3, 3, 5, 3, 7, 2, 4, 8, 1, 8, 2, ...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6712</th>\n",
       "      <td>[9, 6, 1, 3, 9, 7, 8, 3, 4, 1, 7, 1, 8, 2, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>[7, 4, 9, 3, 8, 0, 5, 5, 8, 8, 7, 9, 0, 4, 9, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3079</th>\n",
       "      <td>[3, 8, 0, 9, 3, 3, 1, 5, 8, 3, 4, 2, 8, 2, 9, ...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>[2, 1, 1, 6, 1, 6, 8, 9, 6, 0, 3, 0, 6, 6, 1, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657</th>\n",
       "      <td>[1, 8, 3, 3, 4, 8, 2, 7, 4, 5, 1, 4, 4, 7, 9, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5444</th>\n",
       "      <td>[8, 1, 5, 0, 6, 9, 3, 8, 5, 3, 9, 4, 8, 5, 6, ...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sample       outcome\n",
       "4812  [8, 0, 3, 6, 0, 6, 3, 7, 1, 9, 9, 7, 9, 0, 0, ...  [0, 0, 1, 0]\n",
       "2635  [6, 3, 5, 3, 3, 2, 4, 6, 3, 0, 1, 8, 4, 4, 6, ...  [0, 1, 0, 0]\n",
       "5445  [9, 2, 8, 1, 8, 8, 8, 7, 6, 2, 5, 3, 2, 0, 8, ...  [0, 0, 1, 0]\n",
       "5948  [2, 3, 8, 2, 7, 1, 7, 7, 1, 2, 8, 1, 0, 6, 4, ...  [0, 0, 1, 0]\n",
       "6061  [2, 8, 9, 8, 2, 8, 7, 7, 4, 2, 9, 7, 3, 3, 2, ...  [0, 0, 0, 1]\n",
       "3028  [6, 1, 5, 2, 7, 2, 0, 9, 7, 8, 0, 3, 2, 3, 2, ...  [0, 1, 0, 0]\n",
       "6503  [2, 6, 0, 5, 8, 3, 0, 0, 6, 8, 9, 5, 8, 5, 2, ...  [0, 0, 0, 1]\n",
       "1354  [1, 1, 7, 8, 2, 9, 1, 0, 9, 7, 3, 9, 8, 3, 2, ...  [1, 0, 0, 0]\n",
       "3823  [1, 8, 7, 0, 7, 7, 5, 4, 3, 3, 6, 8, 5, 7, 5, ...  [0, 1, 0, 0]\n",
       "4428  [2, 2, 5, 2, 8, 3, 7, 2, 2, 2, 8, 0, 3, 9, 5, ...  [0, 0, 1, 0]\n",
       "6845  [2, 3, 4, 6, 5, 3, 2, 3, 6, 0, 1, 4, 3, 0, 4, ...  [0, 0, 0, 1]\n",
       "7378  [1, 3, 8, 4, 1, 5, 2, 3, 2, 8, 8, 9, 1, 5, 6, ...  [0, 0, 0, 1]\n",
       "6854  [6, 8, 5, 2, 1, 4, 7, 4, 2, 6, 3, 2, 0, 1, 1, ...  [0, 0, 0, 1]\n",
       "4221  [9, 3, 3, 5, 3, 3, 5, 3, 7, 2, 4, 8, 1, 8, 2, ...  [0, 0, 1, 0]\n",
       "6712  [9, 6, 1, 3, 9, 7, 8, 3, 4, 1, 7, 1, 8, 2, 0, ...  [0, 0, 0, 1]\n",
       "760   [7, 4, 9, 3, 8, 0, 5, 5, 8, 8, 7, 9, 0, 4, 9, ...  [1, 0, 0, 0]\n",
       "3079  [3, 8, 0, 9, 3, 3, 1, 5, 8, 3, 4, 2, 8, 2, 9, ...  [0, 1, 0, 0]\n",
       "1307  [2, 1, 1, 6, 1, 6, 8, 9, 6, 0, 3, 0, 6, 6, 1, ...  [1, 0, 0, 0]\n",
       "1657  [1, 8, 3, 3, 4, 8, 2, 7, 4, 5, 1, 4, 4, 7, 9, ...  [1, 0, 0, 0]\n",
       "5444  [8, 1, 5, 0, 6, 9, 3, 8, 5, 3, 9, 4, 8, 5, 6, ...  [0, 0, 1, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #1\n",
    "\n",
    "# There should be 'sample' and 'outcome' columns\n",
    "# 'sample' contains all phone numbers in period broken down into individual digits\n",
    "# and combined\n",
    "# 'outcome' is 0 or 1 for binary\n",
    "# Also there should be mixed 0 and 1 since the samples have been randomize\n",
    "\n",
    "df_random_with_mid_ooo_2000_2000_2000_2000_sample_number_50.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #2\n",
    "\n",
    "# 25% or 2000 samples should be 0,0,0,1\n",
    "np.count_nonzero(df_random_with_mid_ooo_2000_2000_2000_2000_sample_number_50['outcome'].apply(lambda x: x == [0,0,0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "750"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #3\n",
    "\n",
    "# The 'sample' length should be same as SINGLE_SAMPLE_SIZE_50 = BATCH_NUMBER_COUNT_50 * NUMBER_SIZE\n",
    "len(df_random_with_mid_ooo_2000_2000_2000_2000_sample_number_50['sample'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(X):<class 'list'>, len(X):8000, len(X[0]):750\n",
      "df_row_count: 8000\n",
      "single_sample_size_bit:750\n",
      "X_train.shape:(8000, 1, 750, 1), Y_train.shape:(8000, 4)\n",
      "\n",
      "\n",
      "type(Y[0]):<class 'list'>, Y[0]:[0, 0, 1, 0]\n",
      "type(X):<class 'list'>, len(X):40000, len(X[0]):750\n",
      "df_row_count: 40000\n",
      "single_sample_size_bit:750\n",
      "X_train.shape:(40000, 1, 750, 1), Y_train.shape:(40000, 4)\n",
      "\n",
      "\n",
      "type(Y[0]):<class 'list'>, Y[0]:[1, 0, 0, 0]\n",
      "type(X):<class 'list'>, len(X):56000, len(X[0]):750\n",
      "df_row_count: 56000\n",
      "single_sample_size_bit:750\n",
      "X_train.shape:(56000, 1, 750, 1), Y_train.shape:(56000, 4)\n",
      "\n",
      "\n",
      "type(Y[0]):<class 'list'>, Y[0]:[0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Split out dataframe containing both samples and outcome\n",
    "\n",
    "(X_2000_2000_2000_2000_with_mid_ooo_sample_number_50_train, Y_2000_2000_2000_2000_with_mid_ooo_sample_number_50_train) = prepare_train_data(df_random_with_mid_ooo_2000_2000_2000_2000_sample_number_50, SINGLE_SAMPLE_SIZE_50)\n",
    "\n",
    "(X_10000_10000_10000_10000_with_mid_ooo_sample_number_50_train, Y_10000_10000_10000_10000_with_mid_ooo_sample_number_50_train) = prepare_train_data(df_random_with_mid_ooo_10000_10000_10000_10000_sample_number_50, SINGLE_SAMPLE_SIZE_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 1, 750, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For CNN, the dimensions are number of samples, height, width, channel/feature maps\n",
    "# Number of samples: sum of 'attack' and 'no attack' samples\n",
    "# Height: 1\n",
    "# Width: SINGLE_SAMPLE_SIZE_50\n",
    "# Feature maps/Channels: 1\n",
    "X_2000_2000_2000_2000_with_mid_ooo_sample_number_50_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #5\n",
    "\n",
    "# The total rows should be attack and non-attack rows\n",
    "# The wide should be array of 4, since it can be [0,0,0,0], [0,0,0,1], etc.\n",
    "Y_2000_2000_2000_2000_with_mid_ooo_sample_number_50_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Various multi-class CNN models tested\n",
    "\n",
    "def create_cnn_model_G_50():\n",
    "  # Create model\n",
    "  model = Sequential()\n",
    "  # Max number of digits are different in number yet still can be within threshold\n",
    "  digit_diff_size = 2\n",
    "  digit_same_size = NUMBER_SIZE - digit_diff_size\n",
    "  # If the seq is in the middle then parts that are same is split in two\n",
    "  smallest_digit_same_size = math.floor(digit_same_size/2)\n",
    "  # add model layers\n",
    "  number_digit_diff_size = math.floor(NUMBER_SIZE/digit_diff_size)\n",
    "  # 20190126-5000-60% model.add(Conv2D(80, kernel_size=(1,smallest_digit_same_size), strides=(1,1), activation='relu', input_shape=(1,SINGLE_SAMPLE_SIZE_50,1)))\n",
    "  # 20190126-5000-60% model.add(Conv2D(30, kernel_size=(1,NUMBER_SIZE), strides=(1,NUMBER_SIZE), activation='relu'))\n",
    "  model.add(Conv2D(50, kernel_size=(1,smallest_digit_same_size), strides=(1,1), activation='relu', input_shape=(1,SINGLE_SAMPLE_SIZE_50,1)))\n",
    "  model.add(Conv2D(20, kernel_size=(1,NUMBER_SIZE), strides=(1,NUMBER_SIZE), activation='relu'))\n",
    "  #model.add(Conv2D(THRESHOLD_COUNT, kernel_size=(1,NUMBER_SIZE_BIT), activation='relu'))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(BATCH_NUMBER_COUNT_50, activation=\"relu\"))\n",
    "  model.add(Dense(4, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_cnn_model_G_50_coarse():\n",
    "  # Create model\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(50, kernel_size=(1,NUMBER_SIZE), strides=(1,NUMBER_SIZE), activation='relu', input_shape=(1,SINGLE_SAMPLE_SIZE_50,1)))\n",
    "  model.add(Conv2D(20, kernel_size=(1,1), strides=(1,1), activation='relu'))\n",
    "  #model.add(Conv2D(THRESHOLD_COUNT, kernel_size=(1,NUMBER_SIZE_BIT), activation='relu'))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(BATCH_NUMBER_COUNT_50, activation=\"relu\"))\n",
    "  model.add(Dense(4, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_cnn_model_G_50_coarse_2():\n",
    "  # Create model\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(40, kernel_size=(1,NUMBER_SIZE), strides=(1,NUMBER_SIZE), activation='relu', input_shape=(1,SINGLE_SAMPLE_SIZE_50,1)))\n",
    "  model.add(MaxPooling2D(pool_size=(1,1), strides=(1,1)))\n",
    "  model.add(Conv2D(20, kernel_size=(1,1), strides=(1,1), activation='relu'))\n",
    "  #model.add(Conv2D(THRESHOLD_COUNT, kernel_size=(1,NUMBER_SIZE_BIT), activation='relu'))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(BATCH_NUMBER_COUNT_50, activation=\"relu\"))\n",
    "  model.add(Dense(4, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_cnn_model_G_50_coarse_3():\n",
    "  # Create model\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(50, kernel_size=(1,NUMBER_SIZE), strides=(1,NUMBER_SIZE), activation='relu', input_shape=(1,SINGLE_SAMPLE_SIZE_50,1)))\n",
    "  model.add(MaxPooling2D(pool_size=(1,1), strides=(1,1)))\n",
    "  model.add(Conv2D(30, kernel_size=(1,1), strides=(1,1), activation='relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(1,1), strides=(1,1)))\n",
    "  #model.add(Conv2D(THRESHOLD_COUNT, kernel_size=(1,NUMBER_SIZE_BIT), activation='relu'))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(BATCH_NUMBER_COUNT_50, activation=\"relu\"))\n",
    "  model.add(Dense(4, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_cnn_model_G_50_coarse_4():\n",
    "  # Create model\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(50, kernel_size=(1,NUMBER_SIZE), strides=(1,NUMBER_SIZE), activation='relu', input_shape=(1,SINGLE_SAMPLE_SIZE_50,1)))\n",
    "  model.add(MaxPooling2D(pool_size=(1,1), strides=(1,1)))\n",
    "  model.add(Conv2D(30, kernel_size=(1,1), strides=(1,1), activation='relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(1,1), strides=(1,1)))\n",
    "  #model.add(Conv2D(THRESHOLD_COUNT, kernel_size=(1,NUMBER_SIZE_BIT), activation='relu'))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(int(BATCH_NUMBER_COUNT_50), activation=\"relu\"))\n",
    "  model.add(Dense(4, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_cnn_model_G_50_mod():\n",
    "  # Create model\n",
    "  model = Sequential()\n",
    "  # Max number of bits are different in number yet still can be within threshold\n",
    "  bit_diff_size = BIT_PER_DIGIT*2\n",
    "  bit_same_size = NUMBER_SIZE_BIT - bit_diff_size\n",
    "  # If the seq is in the middle then parts that are same is split in two\n",
    "  smallest_bit_same_size = int(math.floor(bit_same_size/2) / BIT_PER_DIGIT)\n",
    "  # add model layers\n",
    "  number_bit_diff_size = math.floor(NUMBER_SIZE_BIT/bit_diff_size)\n",
    "  model.add(Conv2D(150, kernel_size=(1,smallest_bit_same_size), strides=(1,BIT_PER_DIGIT), activation='relu', input_shape=(1,SINGLE_SAMPLE_SIZE_BIT_50,1)))\n",
    "  model.add(Conv2D(5, kernel_size=(1,NUMBER_SIZE_BIT ), strides=(1,NUMBER_SIZE_BIT), activation='relu'))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(10 - THRESHOLD_COUNT, activation=\"relu\"))\n",
    "  model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_cnn_model_G_50_maxpool():\n",
    "  # Create model\n",
    "  model = Sequential()\n",
    "  # Max number of bits are different in number yet still can be within threshold\n",
    "  digit_diff_size = 2\n",
    "  digit_same_size = NUMBER_SIZE - digit_diff_size\n",
    "  # If the seq is in the middle then parts that are same is split in two\n",
    "  smallest_digit_same_size = math.floor(digit_same_size/2)\n",
    "  # add model layers\n",
    "  number_digit_diff_size = math.floor(NUMBER_SIZE/digit_diff_size)\n",
    "  model.add(Conv2D(50, kernel_size=(1,smallest_digit_same_size), strides=(1,1), activation='relu', input_shape=(1,SINGLE_SAMPLE_SIZE_50,1)))\n",
    "  model.add(Conv2D(20, kernel_size=(1,NUMBER_SIZE), strides=(1,NUMBER_SIZE), activation='relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(1,1), strides=(1,1)))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(BATCH_NUMBER_COUNT_50, activation=\"relu\"))\n",
    "  model.add(Dense(4, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_cnn_model_G_50_avepool():\n",
    "  # Create model\n",
    "  model = Sequential()\n",
    "  # Max number of bits are different in number yet still can be within threshold\n",
    "  digit_diff_size = 2\n",
    "  digit_same_size = NUMBER_SIZE - digit_diff_size\n",
    "  # If the seq is in the middle then parts that are same is split in two\n",
    "  smallest_digit_same_size = math.floor(digit_same_size/2)\n",
    "  # add model layers\n",
    "  number_digit_diff_size = math.floor(NUMBER_SIZE/digit_diff_size)\n",
    "  model.add(Conv2D(50, kernel_size=(1,smallest_digit_same_size), strides=(1,1), activation='relu', input_shape=(1,SINGLE_SAMPLE_SIZE_50,1)))\n",
    "  model.add(AveragePooling2D(pool_size=(1,NUMBER_SIZE), strides=(1,NUMBER_SIZE)))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(BATCH_NUMBER_COUNT_50, activation=\"relu\"))\n",
    "  model.add(Dense(4, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/50\n",
      "32000/32000 [==============================] - 23s - loss: 1.2299 - acc: 0.3649 - val_loss: 0.9632 - val_acc: 0.5354\n",
      "Epoch 2/50\n",
      "32000/32000 [==============================] - 22s - loss: 0.9112 - acc: 0.5708 - val_loss: 0.8406 - val_acc: 0.6068\n",
      "Epoch 3/50\n",
      "32000/32000 [==============================] - 22s - loss: 0.8415 - acc: 0.6074 - val_loss: 0.8159 - val_acc: 0.6276\n",
      "Epoch 4/50\n",
      "32000/32000 [==============================] - 22s - loss: 0.8052 - acc: 0.6349 - val_loss: 0.7744 - val_acc: 0.6544\n",
      "Epoch 5/50\n",
      "32000/32000 [==============================] - 21s - loss: 0.7834 - acc: 0.6457 - val_loss: 0.7874 - val_acc: 0.6569\n",
      "Epoch 6/50\n",
      "32000/32000 [==============================] - 22s - loss: 0.7686 - acc: 0.6544 - val_loss: 0.7573 - val_acc: 0.6614\n",
      "Epoch 7/50\n",
      "32000/32000 [==============================] - 21s - loss: 0.7577 - acc: 0.6565 - val_loss: 0.7614 - val_acc: 0.6588\n",
      "Epoch 8/50\n",
      "32000/32000 [==============================] - 21s - loss: 0.7483 - acc: 0.6636 - val_loss: 0.7615 - val_acc: 0.6465\n",
      "Epoch 9/50\n",
      "32000/32000 [==============================] - 21s - loss: 0.7384 - acc: 0.6707 - val_loss: 0.7325 - val_acc: 0.6738\n",
      "Epoch 10/50\n",
      "32000/32000 [==============================] - 21s - loss: 0.7285 - acc: 0.6747 - val_loss: 0.7382 - val_acc: 0.6726\n",
      "Epoch 11/50\n",
      "32000/32000 [==============================] - 22s - loss: 0.7178 - acc: 0.6802 - val_loss: 0.7465 - val_acc: 0.6709\n",
      "Epoch 12/50\n",
      "32000/32000 [==============================] - 20s - loss: 0.7071 - acc: 0.6886 - val_loss: 0.7178 - val_acc: 0.6805\n",
      "Epoch 13/50\n",
      "32000/32000 [==============================] - 21s - loss: 0.6960 - acc: 0.6921 - val_loss: 0.7360 - val_acc: 0.6745\n",
      "Epoch 14/50\n",
      "32000/32000 [==============================] - 21s - loss: 0.6859 - acc: 0.6982 - val_loss: 0.7158 - val_acc: 0.6896\n",
      "Epoch 15/50\n",
      "32000/32000 [==============================] - 21s - loss: 0.6741 - acc: 0.7048 - val_loss: 0.7204 - val_acc: 0.6873\n",
      "Epoch 16/50\n",
      "32000/32000 [==============================] - 21s - loss: 0.6529 - acc: 0.7142 - val_loss: 0.7121 - val_acc: 0.6916\n",
      "Epoch 17/50\n",
      "32000/32000 [==============================] - 21s - loss: 0.6436 - acc: 0.7202 - val_loss: 0.7100 - val_acc: 0.6958\n",
      "Epoch 18/50\n",
      "32000/32000 [==============================] - 21s - loss: 0.6274 - acc: 0.7300 - val_loss: 0.6973 - val_acc: 0.7019\n",
      "Epoch 19/50\n",
      "32000/32000 [==============================] - 21s - loss: 0.6155 - acc: 0.7341 - val_loss: 0.6923 - val_acc: 0.7048\n",
      "Epoch 20/50\n",
      "32000/32000 [==============================] - 21s - loss: 0.6069 - acc: 0.7406 - val_loss: 0.7281 - val_acc: 0.6921\n",
      "Epoch 21/50\n",
      "32000/32000 [==============================] - 21s - loss: 0.5944 - acc: 0.7446 - val_loss: 0.7480 - val_acc: 0.6886\n",
      "Epoch 22/50\n",
      "32000/32000 [==============================] - 21s - loss: 0.5869 - acc: 0.7500 - val_loss: 0.7077 - val_acc: 0.7011\n",
      "Epoch 23/50\n",
      "32000/32000 [==============================] - 21s - loss: 0.5773 - acc: 0.7523 - val_loss: 0.7411 - val_acc: 0.6933\n",
      "Epoch 24/50\n",
      "32000/32000 [==============================] - 21s - loss: 0.5651 - acc: 0.7614 - val_loss: 0.6990 - val_acc: 0.7125\n",
      "Epoch 25/50\n",
      "32000/32000 [==============================] - 21s - loss: 0.5528 - acc: 0.7620 - val_loss: 0.6769 - val_acc: 0.7136\n",
      "Epoch 26/50\n",
      "32000/32000 [==============================] - 21s - loss: 0.5456 - acc: 0.7673 - val_loss: 0.7053 - val_acc: 0.7069\n",
      "Epoch 27/50\n",
      "32000/32000 [==============================] - 21s - loss: 0.5331 - acc: 0.7743 - val_loss: 0.7021 - val_acc: 0.7069\n",
      "Epoch 28/50\n",
      "32000/32000 [==============================] - 21s - loss: 0.5264 - acc: 0.7800 - val_loss: 0.7220 - val_acc: 0.7120\n",
      "Epoch 29/50\n",
      "32000/32000 [==============================] - 21s - loss: 0.5179 - acc: 0.7810 - val_loss: 0.7536 - val_acc: 0.7034\n",
      "Epoch 30/50\n",
      "32000/32000 [==============================] - 21s - loss: 0.5094 - acc: 0.7833 - val_loss: 0.7026 - val_acc: 0.7146\n",
      "Epoch 31/50\n",
      "32000/32000 [==============================] - 76s - loss: 0.5019 - acc: 0.7888 - val_loss: 0.7241 - val_acc: 0.7079\n",
      "Epoch 32/50\n",
      "32000/32000 [==============================] - 23s - loss: 0.4985 - acc: 0.7892 - val_loss: 0.7297 - val_acc: 0.7128\n",
      "Epoch 33/50\n",
      "32000/32000 [==============================] - 24s - loss: 0.4919 - acc: 0.7912 - val_loss: 0.7445 - val_acc: 0.7190\n",
      "Epoch 34/50\n",
      "32000/32000 [==============================] - 24s - loss: 0.4823 - acc: 0.7958 - val_loss: 0.7196 - val_acc: 0.7106\n",
      "Epoch 35/50\n",
      "32000/32000 [==============================] - 24s - loss: 0.4789 - acc: 0.7991 - val_loss: 0.7527 - val_acc: 0.7146\n",
      "Epoch 36/50\n",
      "32000/32000 [==============================] - 23s - loss: 0.4725 - acc: 0.8002 - val_loss: 0.7717 - val_acc: 0.7129\n",
      "Epoch 37/50\n",
      "32000/32000 [==============================] - 23s - loss: 0.4659 - acc: 0.8036 - val_loss: 0.7613 - val_acc: 0.7069\n",
      "Epoch 38/50\n",
      "32000/32000 [==============================] - 23s - loss: 0.4543 - acc: 0.8097 - val_loss: 0.7637 - val_acc: 0.7131\n",
      "Epoch 39/50\n",
      "32000/32000 [==============================] - 23s - loss: 0.4512 - acc: 0.8106 - val_loss: 0.7708 - val_acc: 0.7116\n",
      "Epoch 40/50\n",
      "32000/32000 [==============================] - 23s - loss: 0.4445 - acc: 0.8132 - val_loss: 0.7926 - val_acc: 0.7144\n",
      "Epoch 41/50\n",
      "32000/32000 [==============================] - 23s - loss: 0.4413 - acc: 0.8145 - val_loss: 0.8566 - val_acc: 0.6840\n",
      "Epoch 42/50\n",
      "32000/32000 [==============================] - 23s - loss: 0.4307 - acc: 0.8191 - val_loss: 0.7764 - val_acc: 0.7174\n",
      "Epoch 43/50\n",
      "32000/32000 [==============================] - 22s - loss: 0.4292 - acc: 0.8200 - val_loss: 0.8300 - val_acc: 0.7038\n",
      "Epoch 44/50\n",
      "32000/32000 [==============================] - 22s - loss: 0.4209 - acc: 0.8230 - val_loss: 0.8393 - val_acc: 0.7131\n",
      "Epoch 45/50\n",
      "32000/32000 [==============================] - 22s - loss: 0.4154 - acc: 0.8258 - val_loss: 0.8502 - val_acc: 0.6990\n",
      "Epoch 46/50\n",
      "32000/32000 [==============================] - 23s - loss: 0.4131 - acc: 0.8283 - val_loss: 0.8235 - val_acc: 0.7134\n",
      "Epoch 47/50\n",
      "32000/32000 [==============================] - 23s - loss: 0.4008 - acc: 0.8347 - val_loss: 0.8445 - val_acc: 0.7143\n",
      "Epoch 48/50\n",
      "32000/32000 [==============================] - 23s - loss: 0.3885 - acc: 0.8388 - val_loss: 0.8316 - val_acc: 0.7203\n",
      "Epoch 49/50\n",
      "32000/32000 [==============================] - 23s - loss: 0.3783 - acc: 0.8441 - val_loss: 0.8359 - val_acc: 0.7154\n",
      "Epoch 50/50\n",
      "32000/32000 [==============================] - 23s - loss: 0.3744 - acc: 0.8459 - val_loss: 0.8743 - val_acc: 0.7145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11a469d68>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The best is perhaps: create_cnn_model_G_50_coarse_3\n",
    "\n",
    "cnn_model_G_50_coarse_3 = create_cnn_model_G_50_coarse_3()\n",
    "\n",
    "cnn_model_G_50_coarse_3.fit(X_10000_10000_10000_10000_with_mid_ooo_sample_number_50_train, Y_10000_10000_10000_10000_with_mid_ooo_sample_number_50_train, validation_split=0.2, batch_size=10, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7776/8000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.718125"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_arr = cnn_model_G_50_coarse_3.predict_classes(X_2000_2000_2000_2000_with_mid_ooo_sample_number_50_train)\n",
    "# Each element in 'a' will contain 1/True if prediction matches expected outcome\n",
    "expected_arr = [np.where(r==1)[0][0] for r in Y_2000_2000_2000_2000_with_mid_ooo_sample_number_50_train]\n",
    "a = (predict_arr==expected_arr)\n",
    "# % of correct predictions\n",
    "np.count_nonzero(a)/np.size(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: This is ONLY for saving models\n",
    "model_json = cnn_model_G_50_coarse_3.to_json()\n",
    "with open(\"cnn_model_G_50_coarse-multi_class_non_binary_sparse-tail_heavy-head_heavy_train_mid_ooo_10000-sample_71-pct-20190518.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "cnn_model_G_50_coarse_3.save('cnn_model_G_50_coarse-multi_class_non_binary_sparse-tail_heavy-head_heavy_train_mid_ooo_10000-sample_71-pct-20190518.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Additional measurements to ensure that the accuracy is not skewed by being able to\n",
    "# detect attacks very well of which there are 3 classes (75%)\n",
    "# but not detect non-attacks which is 1 class (25%)\n",
    "# So we measure accurary with data set where 50% is non attack, and 16.67% for\n",
    "# each of the 3 attack classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "cnn_model_G_50_coarse_3 = create_cnn_model_G_50_coarse_3()\n",
    "cnn_model_G_50_coarse_3.load_weights('cnn_model_G_50_coarse-multi_class_non_binary_sparse-tail_heavy-head_heavy_train_mid_ooo_10000-sample_71-pct-20190518.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data file:data_no_sequence_6000_sample_number_50.csv, data.shape (should be (6000, 50)): (6000, 50)\n",
      "data file:data_sequence_sparse_ooo_mid_combo_2000_sample_number_50.csv, data.shape (should be (2000, 50)): (2000, 50)\n",
      "data file:data_sequence_head_heavy_ooo_mid_combo_2000_sample_number_50.csv, data.shape (should be (2000, 50)): (2000, 50)\n",
      "data file:data_sequence_tail_heavy_ooo_mid_combo_2000_sample_number_50.csv, data.shape (should be (2000, 50)): (2000, 50)\n",
      "outcome_value:[1, 0, 0, 0], outcome_values:[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
      "outcome_value:[0, 1, 0, 0], outcome_values:[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
      "outcome_value:[0, 0, 1, 0], outcome_values:[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
      "outcome_value:[0, 0, 0, 1], outcome_values:[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "df_random_with_mid_ooo_6000_2000_2000_2000_sample_number_50 = prepare_data(['data_no_sequence_6000_sample_number_50.csv', 'data_sequence_sparse_ooo_mid_combo_2000_sample_number_50.csv', 'data_sequence_head_heavy_ooo_mid_combo_2000_sample_number_50.csv', 'data_sequence_tail_heavy_ooo_mid_combo_2000_sample_number_50.csv'], [(6000,50), (2000,50), (2000,50), (2000,50)], [CLASSES[\"none\"], CLASSES[\"sparse\"], CLASSES[\"head-heavy\"], CLASSES[\"tail-heavy\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7767</th>\n",
       "      <td>[9, 3, 7, 4, 1, 9, 5, 7, 3, 7, 5, 1, 2, 4, 5, ...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7133</th>\n",
       "      <td>[8, 3, 5, 1, 5, 4, 2, 4, 6, 2, 8, 9, 1, 4, 9, ...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11191</th>\n",
       "      <td>[8, 4, 9, 5, 2, 9, 5, 1, 4, 7, 9, 9, 9, 0, 4, ...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3967</th>\n",
       "      <td>[7, 7, 2, 1, 7, 5, 2, 5, 2, 8, 7, 6, 4, 2, 5, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>[6, 4, 7, 8, 4, 7, 4, 2, 2, 2, 9, 7, 7, 8, 3, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2270</th>\n",
       "      <td>[7, 5, 7, 9, 4, 5, 9, 8, 3, 2, 1, 2, 5, 3, 1, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10134</th>\n",
       "      <td>[1, 1, 9, 4, 6, 0, 9, 8, 9, 2, 6, 2, 9, 0, 3, ...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10625</th>\n",
       "      <td>[4, 0, 6, 3, 7, 4, 5, 8, 3, 1, 1, 7, 8, 6, 9, ...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4479</th>\n",
       "      <td>[4, 8, 5, 5, 9, 4, 7, 1, 1, 1, 4, 6, 2, 4, 5, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4275</th>\n",
       "      <td>[9, 2, 9, 7, 3, 5, 0, 7, 6, 0, 8, 4, 3, 6, 1, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3382</th>\n",
       "      <td>[1, 5, 1, 8, 8, 2, 9, 7, 7, 0, 5, 3, 0, 6, 3, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>[1, 6, 2, 4, 8, 5, 2, 0, 6, 8, 6, 8, 0, 5, 4, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>[4, 6, 2, 5, 4, 7, 8, 4, 8, 2, 7, 2, 6, 3, 7, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2760</th>\n",
       "      <td>[1, 4, 9, 3, 7, 1, 2, 4, 2, 5, 5, 6, 5, 3, 6, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7543</th>\n",
       "      <td>[5, 6, 4, 7, 0, 4, 5, 5, 0, 0, 3, 6, 9, 4, 5, ...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3186</th>\n",
       "      <td>[2, 7, 3, 2, 6, 2, 2, 7, 5, 2, 5, 1, 6, 2, 7, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6626</th>\n",
       "      <td>[8, 0, 8, 8, 2, 7, 2, 1, 8, 6, 0, 4, 6, 7, 6, ...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8631</th>\n",
       "      <td>[9, 6, 6, 4, 6, 0, 5, 3, 5, 1, 9, 9, 3, 6, 5, ...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>[7, 7, 3, 4, 0, 6, 2, 5, 9, 8, 4, 7, 2, 6, 3, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>[4, 1, 9, 6, 5, 9, 8, 4, 3, 7, 3, 6, 9, 7, 9, ...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  sample       outcome\n",
       "7767   [9, 3, 7, 4, 1, 9, 5, 7, 3, 7, 5, 1, 2, 4, 5, ...  [0, 1, 0, 0]\n",
       "7133   [8, 3, 5, 1, 5, 4, 2, 4, 6, 2, 8, 9, 1, 4, 9, ...  [0, 1, 0, 0]\n",
       "11191  [8, 4, 9, 5, 2, 9, 5, 1, 4, 7, 9, 9, 9, 0, 4, ...  [0, 0, 0, 1]\n",
       "3967   [7, 7, 2, 1, 7, 5, 2, 5, 2, 8, 7, 6, 4, 2, 5, ...  [1, 0, 0, 0]\n",
       "969    [6, 4, 7, 8, 4, 7, 4, 2, 2, 2, 9, 7, 7, 8, 3, ...  [1, 0, 0, 0]\n",
       "2270   [7, 5, 7, 9, 4, 5, 9, 8, 3, 2, 1, 2, 5, 3, 1, ...  [1, 0, 0, 0]\n",
       "10134  [1, 1, 9, 4, 6, 0, 9, 8, 9, 2, 6, 2, 9, 0, 3, ...  [0, 0, 0, 1]\n",
       "10625  [4, 0, 6, 3, 7, 4, 5, 8, 3, 1, 1, 7, 8, 6, 9, ...  [0, 0, 0, 1]\n",
       "4479   [4, 8, 5, 5, 9, 4, 7, 1, 1, 1, 4, 6, 2, 4, 5, ...  [1, 0, 0, 0]\n",
       "4275   [9, 2, 9, 7, 3, 5, 0, 7, 6, 0, 8, 4, 3, 6, 1, ...  [1, 0, 0, 0]\n",
       "3382   [1, 5, 1, 8, 8, 2, 9, 7, 7, 0, 5, 3, 0, 6, 3, ...  [1, 0, 0, 0]\n",
       "1272   [1, 6, 2, 4, 8, 5, 2, 0, 6, 8, 6, 8, 0, 5, 4, ...  [1, 0, 0, 0]\n",
       "1798   [4, 6, 2, 5, 4, 7, 8, 4, 8, 2, 7, 2, 6, 3, 7, ...  [1, 0, 0, 0]\n",
       "2760   [1, 4, 9, 3, 7, 1, 2, 4, 2, 5, 5, 6, 5, 3, 6, ...  [1, 0, 0, 0]\n",
       "7543   [5, 6, 4, 7, 0, 4, 5, 5, 0, 0, 3, 6, 9, 4, 5, ...  [0, 1, 0, 0]\n",
       "3186   [2, 7, 3, 2, 6, 2, 2, 7, 5, 2, 5, 1, 6, 2, 7, ...  [1, 0, 0, 0]\n",
       "6626   [8, 0, 8, 8, 2, 7, 2, 1, 8, 6, 0, 4, 6, 7, 6, ...  [0, 1, 0, 0]\n",
       "8631   [9, 6, 6, 4, 6, 0, 5, 3, 5, 1, 9, 9, 3, 6, 5, ...  [0, 0, 1, 0]\n",
       "1920   [7, 7, 3, 4, 0, 6, 2, 5, 9, 8, 4, 7, 2, 6, 3, ...  [1, 0, 0, 0]\n",
       "4000   [4, 1, 9, 6, 5, 9, 8, 4, 3, 7, 3, 6, 9, 7, 9, ...  [1, 0, 0, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #1\n",
    "\n",
    "# There should be 'sample' and 'outcome' columns\n",
    "# 'sample' contains all phone numbers in period broken down into individual digits\n",
    "# and combined\n",
    "# 'outcome' is 0 or 1 for binary\n",
    "# Also there should be mixed 0 and 1 since the samples have been randomize\n",
    "\n",
    "df_random_with_mid_ooo_6000_2000_2000_2000_sample_number_50.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #2a\n",
    "\n",
    "# 50% or 6000 samples should be 1,0,0,0\n",
    "np.count_nonzero(df_random_with_mid_ooo_6000_2000_2000_2000_sample_number_50['outcome'].apply(lambda x: x == [1,0,0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #2b\n",
    "\n",
    "# 2000 samples should be 0,0,0,1, i.e., other classes\n",
    "np.count_nonzero(df_random_with_mid_ooo_6000_2000_2000_2000_sample_number_50['outcome'].apply(lambda x: x == [0,0,0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "750"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #3\n",
    "\n",
    "# The 'sample' length should be same as SINGLE_SAMPLE_SIZE_50 = BATCH_NUMBER_COUNT_50 * NUMBER_SIZE\n",
    "len(df_random_with_mid_ooo_6000_2000_2000_2000_sample_number_50['sample'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(X):<class 'list'>, len(X):12000, len(X[0]):750\n",
      "df_row_count: 12000\n",
      "single_sample_size_bit:750\n",
      "X_train.shape:(12000, 1, 750, 1), Y_train.shape:(12000, 4)\n",
      "\n",
      "\n",
      "type(Y[0]):<class 'list'>, Y[0]:[0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "(X_6000_2000_2000_2000_with_mid_ooo_sample_number_50_train, Y_6000_2000_2000_2000_with_mid_ooo_sample_number_50_train) = prepare_train_data(df_random_with_mid_ooo_6000_2000_2000_2000_sample_number_50, SINGLE_SAMPLE_SIZE_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 1, 750, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For CNN, the dimensions are number of samples, height, width, channel/feature maps\n",
    "# Number of samples: sum of 'attack' and 'no attack' samples\n",
    "# Height: 1\n",
    "# Width: SINGLE_SAMPLE_SIZE_50\n",
    "# Feature maps/Channels: 1\n",
    "X_6000_2000_2000_2000_with_mid_ooo_sample_number_50_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #5\n",
    "\n",
    "# The total rows should be attack and non-attack rows\n",
    "# The wide should be array of 4, since it can be [0,0,0,0], [0,0,0,1], etc.\n",
    "Y_6000_2000_2000_2000_with_mid_ooo_sample_number_50_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11872/12000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7189166666666666"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_arr = cnn_model_G_50_coarse_3.predict_classes(X_6000_2000_2000_2000_with_mid_ooo_sample_number_50_train)\n",
    "# Each element in 'a' will contain 1/True if prediction matches expected outcome\n",
    "expected_arr = [np.where(r==1)[0][0] for r in Y_6000_2000_2000_2000_with_mid_ooo_sample_number_50_train]\n",
    "a = (predict_arr==expected_arr)\n",
    "# % of correct predictions\n",
    "np.count_nonzero(a)/np.size(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
