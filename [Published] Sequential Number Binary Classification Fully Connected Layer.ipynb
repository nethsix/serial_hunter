{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras as kr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "from functools import reduce\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D, Input\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUMBER_SIZE = 15\n",
    "THRESHOLD_COUNT = 5\n",
    "\n",
    "BATCH_NUMBER_COUNT_50 = 50\n",
    "SINGLE_SAMPLE_SIZE_50 = BATCH_NUMBER_COUNT_50 * NUMBER_SIZE\n",
    "INPUT_DIM = SINGLE_SAMPLE_SIZE_50\n",
    "\n",
    "CLASSES = { \"none\": 0, \"attack\": 1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_single_sample_from_array(numbers):\n",
    "  # Treat the list of phone numbers as a single huge list digits\n",
    "  # pretty much like greyscale images, etc.\n",
    "  digit_str = ''.join([str(i) for i in numbers])\n",
    "  # Now create an image-like array\n",
    "  return list(digit_str)\n",
    "\n",
    "def create_single_sample_from_dataframe(dataframe_row):\n",
    "  return create_single_sample_from_array(np.array(dataframe_row))\n",
    "\n",
    "def combine_dfs(dfs, outcome_values=None, shuffle=True):\n",
    "  # Combine data with sequence with outcome\n",
    "  df_list = []\n",
    "  for idx, df in enumerate(dfs):\n",
    "    data_arr = []\n",
    "    for index, series in df.iterrows():\n",
    "      series_value = [v for _, v in series.iteritems()]\n",
    "      combined_series = create_single_sample_from_array(series_value)\n",
    "      data_arr.append(combined_series)\n",
    "    \n",
    "    df_arr_row_count = len(data_arr)\n",
    "    \n",
    "    outcome_value = outcome_values[idx] if outcome_values else idx\n",
    "    data_outcome_tuples = zip(data_arr, [outcome_value] * df_arr_row_count)\n",
    "\n",
    "    # DEBUG\n",
    "    print(\"outcome_value:%s of all possible outcome_values:%s\" % (outcome_value, outcome_values))\n",
    "\n",
    "    df_list += data_outcome_tuples   \n",
    "    \n",
    "  # Merge data with sequence and no sequence\n",
    "  dataframe = pd.DataFrame(df_list, columns=[\"sample\",\"outcome\"])\n",
    "  # Shuffle (or rather randomly select samples) but 1.0 means all\n",
    "  df_random = dataframe.sample(frac=1) if shuffle else dataframe\n",
    "  return df_random\n",
    "\n",
    "def extract_sample_and_outcome(df, sample_col_name='sample', outcome_col_name='outcome'):\n",
    "  X = [i for i in df[sample_col_name]]\n",
    "  Y = [i for i in df[outcome_col_name]]\n",
    "  return (X, Y)\n",
    "\n",
    "def load_data(filename, expected_shape_tuple):\n",
    "  data = pd.read_csv(filename, header=None)\n",
    "  print(\"data file:%s, data.shape (should be %s): %s\" % (filename, expected_shape_tuple, data.shape))\n",
    "  if data.shape != expected_shape_tuple:\n",
    "    raise ValueError(\"data.shape:%s does not match excpected shape:%s\" % (data.shape, expected_shape_tuple))\n",
    "  return data\n",
    "    \n",
    "def prepare_data(filenames, expected_shape_tuples, outcome_values=None, shuffle=True):\n",
    "  dfs = []\n",
    "  for idx, filename in enumerate(filenames):\n",
    "    dfs.append(load_data(filename, expected_shape_tuples[idx]))\n",
    "\n",
    "  df_all = combine_dfs(dfs, outcome_values, shuffle)\n",
    "  if (df_all.shape[0] != reduce((lambda m, i: m + i.shape[0]), dfs,0)):\n",
    "    raise ValueError(\"There is a problem with combine_dfs. df_all.shape:%s does not match the sum of df_seq:%s and df_no_seq:%s\" % (df_all.shape, df_seq.shape, df_no_seq.shape))\n",
    "  return df_all\n",
    "\n",
    "def prepare_train_data(dataframe, single_sample_size):\n",
    "  df_row_count = dataframe.shape[0]\n",
    "  (X, Y) = extract_sample_and_outcome(dataframe)\n",
    "  X_train = np.array(X)\n",
    "  Y_train = np.array(Y)\n",
    "\n",
    "  print(\"type(X):%s, len(X):%d, len(X[0]):%d\" % (type(X), len(X), len(X[0])))\n",
    "  print(\"df_row_count: %d\" % df_row_count)\n",
    "  print(\"single_sample_size_bit:%d\" % single_sample_size)\n",
    "  print(\"X_train.shape:%s, Y_train.shape:%s\" % (X_train.shape, Y_train.shape))\n",
    "  print(\"\\n\")\n",
    "\n",
    "  print(\"type(Y[0]):%s, Y[0]:%s\" % (type(Y[0]), Y[0]))\n",
    "\n",
    "  return (X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data file:data_no_sequence_2040_sample_number_50.csv, data.shape (should be (2040, 50)): (2040, 50)\n",
      "data file:data_sequence_sparse_head_heavy_tail_heavy_ooo_mid_combo_2040_sample_number_50.csv, data.shape (should be (2040, 50)): (2040, 50)\n",
      "outcome_value:0 of all possible outcome_values:[0, 1]\n",
      "outcome_value:1 of all possible outcome_values:[0, 1]\n",
      "data file:data_no_sequence_10200_sample_number_50.csv, data.shape (should be (10200, 50)): (10200, 50)\n",
      "data file:data_sequence_sparse_head_heavy_tail_heavy_ooo_mid_combo_10200_sample_number_50.csv, data.shape (should be (10200, 50)): (10200, 50)\n",
      "outcome_value:0 of all possible outcome_values:[0, 1]\n",
      "outcome_value:1 of all possible outcome_values:[0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Load data from files into dataframe\n",
    "\n",
    "df_random_2040_2040_combo_sample_number_50 = prepare_data(['data_no_sequence_2040_sample_number_50.csv','data_sequence_sparse_head_heavy_tail_heavy_ooo_mid_combo_2040_sample_number_50.csv'], [(2040,50), (2040,50)], [CLASSES[\"none\"],CLASSES[\"attack\"]]) \n",
    "\n",
    "df_random_10200_10200_combo_sample_number_50 = prepare_data(['data_no_sequence_10200_sample_number_50.csv','data_sequence_sparse_head_heavy_tail_heavy_ooo_mid_combo_10200_sample_number_50.csv'], [(10200,50), (10200,50)], [CLASSES[\"none\"],CLASSES[\"attack\"]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>[2, 9, 3, 2, 8, 4, 3, 2, 7, 5, 2, 5, 0, 7, 2, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2941</th>\n",
       "      <td>[8, 8, 7, 4, 1, 1, 9, 4, 1, 3, 4, 7, 6, 8, 5, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>[8, 0, 9, 6, 5, 3, 3, 2, 0, 6, 4, 7, 2, 2, 5, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2983</th>\n",
       "      <td>[6, 7, 9, 2, 4, 4, 0, 2, 1, 3, 7, 7, 5, 6, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>[3, 8, 1, 2, 7, 4, 6, 1, 8, 8, 3, 3, 4, 7, 5, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sample  outcome\n",
       "978   [2, 9, 3, 2, 8, 4, 3, 2, 7, 5, 2, 5, 0, 7, 2, ...        0\n",
       "2941  [8, 8, 7, 4, 1, 1, 9, 4, 1, 3, 4, 7, 6, 8, 5, ...        1\n",
       "2439  [8, 0, 9, 6, 5, 3, 3, 2, 0, 6, 4, 7, 2, 2, 5, ...        1\n",
       "2983  [6, 7, 9, 2, 4, 4, 0, 2, 1, 3, 7, 7, 5, 6, 1, ...        1\n",
       "1517  [3, 8, 1, 2, 7, 4, 6, 1, 8, 8, 3, 3, 4, 7, 5, ...        0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #1\n",
    "\n",
    "# There should be 'sample' and 'outcome' columns\n",
    "# 'sample' contains all phone numbers in period broken down into individual digits\n",
    "# and combined\n",
    "# 'outcome' is 0 or 1 for binary\n",
    "# Also there should be mixed 0 and 1 since the samples have been randomize\n",
    "df_random_2040_2040_combo_sample_number_50.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2040"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #2\n",
    "\n",
    "# 50% or 2040 samples should be 1\n",
    "np.count_nonzero(df_random_2040_2040_combo_sample_number_50['outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "750"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #3\n",
    "\n",
    "# The 'sample' length should be same as SINGLE_SAMPLE_SIZE_50 = BATCH_NUMBER_COUNT_50 * NUMBER_SIZE\n",
    "len(df_random_2040_2040_combo_sample_number_50['sample'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(X):<class 'list'>, len(X):4080, len(X[0]):750\n",
      "df_row_count: 4080\n",
      "single_sample_size_bit:750\n",
      "X_train.shape:(4080, 750), Y_train.shape:(4080,)\n",
      "\n",
      "\n",
      "type(Y[0]):<class 'numpy.int64'>, Y[0]:0\n",
      "type(X):<class 'list'>, len(X):20400, len(X[0]):750\n",
      "df_row_count: 20400\n",
      "single_sample_size_bit:750\n",
      "X_train.shape:(20400, 750), Y_train.shape:(20400,)\n",
      "\n",
      "\n",
      "type(Y[0]):<class 'numpy.int64'>, Y[0]:0\n"
     ]
    }
   ],
   "source": [
    "# Split out dataframe containing both samples and outcome\n",
    "\n",
    "(X_2040_2040_combo_sample_number_50_train, Y_2040_2040_combo_sample_number_50_train) = prepare_train_data(df_random_2040_2040_combo_sample_number_50, SINGLE_SAMPLE_SIZE_50)\n",
    "\n",
    "(X_10200_10200_combo_sample_number_50_train, Y_10200_10200_combo_sample_number_50_train) = prepare_train_data(df_random_10200_10200_combo_sample_number_50, SINGLE_SAMPLE_SIZE_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4080, 750)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #4\n",
    "\n",
    "# The total rows should be attack and non-attack rows\n",
    "# The wide should be SINGLE_SAMPLE_SIZE_50\n",
    "X_2040_2040_combo_sample_number_50_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4080,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #5\n",
    "\n",
    "# The total rows should be attack and non-attack rows\n",
    "# The wide should be single digit, i.e., 0 or 1\n",
    "Y_2040_2040_combo_sample_number_50_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16320 samples, validate on 4080 samples\n",
      "Epoch 1/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.7315 - acc: 0.4988 - val_loss: 0.6953 - val_acc: 0.5047\n",
      "Epoch 2/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.7014 - acc: 0.4997 - val_loss: 0.6941 - val_acc: 0.5034\n",
      "Epoch 3/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6957 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.5049\n",
      "Epoch 4/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.5001 - val_loss: 0.6932 - val_acc: 0.4958\n",
      "Epoch 5/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4963 - val_loss: 0.6932 - val_acc: 0.4958\n",
      "Epoch 6/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6931 - acc: 0.5022 - val_loss: 0.6931 - val_acc: 0.5051\n",
      "Epoch 7/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4947 - val_loss: 0.6931 - val_acc: 0.5051\n",
      "Epoch 8/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4967 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 9/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4949 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 10/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4999 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 11/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.5001 - val_loss: 0.6933 - val_acc: 0.4956\n",
      "Epoch 12/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.5028 - val_loss: 0.6934 - val_acc: 0.4953\n",
      "Epoch 13/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4977 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 14/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4926 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 15/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4975 - val_loss: 0.6933 - val_acc: 0.4953\n",
      "Epoch 16/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.5013 - val_loss: 0.6931 - val_acc: 0.5049\n",
      "Epoch 17/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4982 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 18/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4951 - val_loss: 0.6932 - val_acc: 0.5051\n",
      "Epoch 19/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4950 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 20/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4940 - val_loss: 0.6932 - val_acc: 0.4958\n",
      "Epoch 21/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.5036 - val_loss: 0.6933 - val_acc: 0.4953\n",
      "Epoch 22/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4973 - val_loss: 0.6932 - val_acc: 0.5049\n",
      "Epoch 23/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4958 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 24/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4995 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 25/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4952 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 26/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4999 - val_loss: 0.6932 - val_acc: 0.4953\n",
      "Epoch 27/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.5009 - val_loss: 0.6933 - val_acc: 0.4953\n",
      "Epoch 28/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4995 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 29/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4909 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 30/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4994 - val_loss: 0.6932 - val_acc: 0.4956\n",
      "Epoch 31/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4979 - val_loss: 0.6932 - val_acc: 0.4953\n",
      "Epoch 32/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4994 - val_loss: 0.6932 - val_acc: 0.4951\n",
      "Epoch 33/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4994 - val_loss: 0.6932 - val_acc: 0.4951\n",
      "Epoch 34/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.5047\n",
      "Epoch 35/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4987 - val_loss: 0.6932 - val_acc: 0.4953\n",
      "Epoch 36/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4955 - val_loss: 0.6932 - val_acc: 0.4951\n",
      "Epoch 37/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4989 - val_loss: 0.6932 - val_acc: 0.4951\n",
      "Epoch 38/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4963 - val_loss: 0.6932 - val_acc: 0.4951\n",
      "Epoch 39/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.5012 - val_loss: 0.6932 - val_acc: 0.4951\n",
      "Epoch 40/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4971 - val_loss: 0.6932 - val_acc: 0.4951\n",
      "Epoch 41/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.5012 - val_loss: 0.6932 - val_acc: 0.5047\n",
      "Epoch 42/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4972 - val_loss: 0.6932 - val_acc: 0.4951\n",
      "Epoch 43/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4982 - val_loss: 0.6932 - val_acc: 0.4951\n",
      "Epoch 44/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4989 - val_loss: 0.6932 - val_acc: 0.4951\n",
      "Epoch 45/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4973 - val_loss: 0.6932 - val_acc: 0.4951\n",
      "Epoch 46/50\n",
      "16320/16320 [==============================] - 6s - loss: 0.6932 - acc: 0.4969 - val_loss: 0.6932 - val_acc: 0.4951\n",
      "Epoch 47/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4966 - val_loss: 0.6932 - val_acc: 0.4951\n",
      "Epoch 48/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4967 - val_loss: 0.6932 - val_acc: 0.4953\n",
      "Epoch 49/50\n",
      "16320/16320 [==============================] - 5s - loss: 0.6932 - acc: 0.4966 - val_loss: 0.6932 - val_acc: 0.4951\n",
      "Epoch 50/50\n",
      "16320/16320 [==============================] - 6s - loss: 0.6932 - acc: 0.5012 - val_loss: 0.6932 - val_acc: 0.4951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x115387eb8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_model = Sequential()\n",
    "basic_model.add(Dense(100, input_dim=INPUT_DIM, activation=\"relu\"))\n",
    "basic_model.add(Dense(64, activation=\"relu\"))\n",
    "basic_model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "basic_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "basic_model.fit(X_10200_10200_combo_sample_number_50_train, Y_10200_10200_combo_sample_number_50_train, validation_split=0.2, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4064/4080 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_arr = basic_model.predict_classes(X_2040_2040_combo_sample_number_50_train)\n",
    "# Each element in 'a' will contain 1/True if prediction matches expected outcome\n",
    "a = (predict_arr == np.array([[r] for r in Y_2040_2040_combo_sample_number_50_train]))\n",
    "# % of correct predictions\n",
    "np.count_nonzero(a)/np.size(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Other models that we tested but did not have improved results\n",
    "\n",
    "def create_baseline():\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(100, input_dim=INPUT_DIM, kernel_initializer='normal', activation='relu'))\n",
    "  model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_model_A():\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(100, input_dim=INPUT_DIM, kernel_initializer='normal', activation='relu'))\n",
    "  model.add(Dense(64, activation=\"relu\"))\n",
    "  model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_model_B():\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(50, input_dim=INPUT_DIM, kernel_initializer='normal', activation='relu'))\n",
    "  model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_model_C():\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(50, input_dim=INPUT_DIM, kernel_initializer='normal', activation='relu'))\n",
    "  model.add(Dense(32, activation=\"relu\"))\n",
    "  model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_model_D():\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(5*NUMBER_SIZE_BIT, input_dim=INPUT_DIM, kernel_initializer='normal', activation='relu'))\n",
    "  model.add(Dense(NUMBER_SIZE_BIT, activation=\"relu\"))\n",
    "  model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_model_E():\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(THRESHOLD_COUNT*NUMBER_SIZE_BIT, input_dim=INPUT_DIM, kernel_initializer='normal', activation='relu'))\n",
    "  model.add(Dense(THRESHOLD_COUNT, activation=\"relu\"))\n",
    "  model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_model_F():\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(INPUT_DIM, input_dim=INPUT_DIM, kernel_initializer='normal', activation='relu'))\n",
    "  model.add(Dense(BATCH_NUMBER_COUNT, activation=\"relu\"))\n",
    "  model.add(Dense(THRESHOLD_COUNT, activation=\"relu\"))\n",
    "  model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_model_G():\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(INPUT_DIM, input_dim=INPUT_DIM, kernel_initializer='normal', activation='relu'))\n",
    "  model.add(Dense(THRESHOLD_COUNT*NUMBER_SIZE, activation=\"relu\"))\n",
    "  model.add(Dense(THRESHOLD_COUNT, activation=\"relu\"))\n",
    "  model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_model_F_1():\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(INPUT_DIM, input_dim=INPUT_DIM, kernel_initializer='normal', activation='relu'))\n",
    "  model.add(Dense(BATCH_NUMBER_COUNT, activation=\"relu\"))\n",
    "  model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_model_H():\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(BATCH_NUMBER_COUNT, input_dim=INPUT_DIM, kernel_initializer='normal', activation='relu'))\n",
    "  model.add(Dense(THRESHOLD_COUNT, activation=\"relu\"))\n",
    "  model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_model_I():\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(BATCH_NUMBER_COUNT, input_dim=INPUT_DIM, kernel_initializer='normal', activation='relu'))\n",
    "  model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6997 - acc: 0.5002    \n",
      "Epoch 2/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6934 - acc: 0.5026    \n",
      "Epoch 3/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6933 - acc: 0.4928    \n",
      "Epoch 4/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6932 - acc: 0.4964    \n",
      "Epoch 5/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6932 - acc: 0.4984    \n",
      "Epoch 6/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6932 - acc: 0.5014    \n",
      "Epoch 7/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6933 - acc: 0.4919    \n",
      "Epoch 8/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6932 - acc: 0.5029    \n",
      "Epoch 9/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6932 - acc: 0.4994    \n",
      "Epoch 10/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6933 - acc: 0.4947    \n",
      "4080/4080 [==============================] - 1s     \n",
      "Epoch 1/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.7038 - acc: 0.4955    \n",
      "Epoch 2/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6937 - acc: 0.4952    \n",
      "Epoch 3/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6932 - acc: 0.4988    \n",
      "Epoch 4/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6932 - acc: 0.4941    \n",
      "Epoch 5/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6932 - acc: 0.4958    \n",
      "Epoch 6/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6932 - acc: 0.4925    \n",
      "Epoch 7/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6932 - acc: 0.5000    \n",
      "Epoch 8/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6932 - acc: 0.5023    \n",
      "Epoch 9/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6932 - acc: 0.5013    \n",
      "Epoch 10/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6932 - acc: 0.4960    \n",
      "4065/4080 [============================>.] - ETA: 0sEpoch 1/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.7061 - acc: 0.5028    \n",
      "Epoch 2/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6937 - acc: 0.5001    \n",
      "Epoch 3/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6933 - acc: 0.4949    \n",
      "Epoch 4/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6932 - acc: 0.4944    \n",
      "Epoch 5/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6933 - acc: 0.4930    \n",
      "Epoch 6/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6932 - acc: 0.5014    \n",
      "Epoch 7/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6932 - acc: 0.5013    \n",
      "Epoch 8/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6933 - acc: 0.5002    \n",
      "Epoch 9/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6932 - acc: 0.4939    \n",
      "Epoch 10/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6932 - acc: 0.4953    \n",
      "3970/4080 [============================>.] - ETA: 0sEpoch 1/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.7005 - acc: 0.4982    \n",
      "Epoch 2/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6937 - acc: 0.4965    \n",
      "Epoch 3/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6932 - acc: 0.5026    \n",
      "Epoch 4/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6932 - acc: 0.4960    \n",
      "Epoch 5/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6932 - acc: 0.5013    \n",
      "Epoch 6/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6932 - acc: 0.5013    \n",
      "Epoch 7/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6932 - acc: 0.5022    \n",
      "Epoch 8/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6933 - acc: 0.4986    \n",
      "Epoch 9/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6933 - acc: 0.4986    \n",
      "Epoch 10/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6932 - acc: 0.5000    \n",
      "4055/4080 [============================>.] - ETA: 0sEpoch 1/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.7027 - acc: 0.5000    \n",
      "Epoch 2/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6936 - acc: 0.4998    \n",
      "Epoch 3/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6931 - acc: 0.5043    \n",
      "Epoch 4/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6932 - acc: 0.4967    \n",
      "Epoch 5/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6933 - acc: 0.4916    \n",
      "Epoch 6/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6932 - acc: 0.4956    \n",
      "Epoch 7/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6932 - acc: 0.5015    \n",
      "Epoch 8/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6932 - acc: 0.4974    \n",
      "Epoch 9/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6932 - acc: 0.4962    \n",
      "Epoch 10/10\n",
      "16320/16320 [==============================] - 13s - loss: 0.6932 - acc: 0.4972    \n",
      "4020/4080 [============================>.] - ETA: 0sResults: 50.00% (0.02%)\n"
     ]
    }
   ],
   "source": [
    "# You can evaluate other models that we created but the results do NOT improve\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Evaluate baseline model\n",
    "seed = 7\n",
    "estimator = KerasClassifier(build_fn=create_baseline, epochs=10, batch_size=5, verbose=1)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X_10200_10200_combo_sample_number_50_train, Y_10200_10200_combo_sample_number_50_train, cv=kfold)\n",
    "\n",
    "print(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.7004 - acc: 0.4936    \n",
      "Epoch 2/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6935 - acc: 0.5029    \n",
      "Epoch 3/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6933 - acc: 0.4936    \n",
      "Epoch 4/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6932 - acc: 0.5031    \n",
      "Epoch 5/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6932 - acc: 0.4991    \n",
      "Epoch 6/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6932 - acc: 0.4935    \n",
      "Epoch 7/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6932 - acc: 0.5007    \n",
      "Epoch 8/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6932 - acc: 0.4974    \n",
      "Epoch 9/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6932 - acc: 0.5039    \n",
      "Epoch 10/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6932 - acc: 0.5006    \n",
      "3960/4080 [============================>.] - ETA: 0sEpoch 1/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6998 - acc: 0.5010    \n",
      "Epoch 2/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6933 - acc: 0.5055    \n",
      "Epoch 3/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6933 - acc: 0.4949    \n",
      "Epoch 4/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6932 - acc: 0.5013    \n",
      "Epoch 5/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6932 - acc: 0.4972    \n",
      "Epoch 6/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6932 - acc: 0.5019    \n",
      "Epoch 7/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6932 - acc: 0.4983    \n",
      "Epoch 8/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6932 - acc: 0.4999    \n",
      "Epoch 9/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6933 - acc: 0.4942    \n",
      "Epoch 10/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6932 - acc: 0.4988    \n",
      "3990/4080 [============================>.] - ETA: 0sEpoch 1/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6977 - acc: 0.4987    \n",
      "Epoch 2/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6933 - acc: 0.4966    \n",
      "Epoch 3/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6932 - acc: 0.4999    \n",
      "Epoch 4/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6932 - acc: 0.4972    \n",
      "Epoch 5/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6932 - acc: 0.4945    \n",
      "Epoch 6/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6932 - acc: 0.4966    \n",
      "Epoch 7/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6932 - acc: 0.4943    \n",
      "Epoch 8/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6932 - acc: 0.4982    \n",
      "Epoch 9/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6932 - acc: 0.5006    \n",
      "Epoch 10/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6933 - acc: 0.4941    \n",
      "4035/4080 [============================>.] - ETA: 0sEpoch 1/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6997 - acc: 0.4982    \n",
      "Epoch 2/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6934 - acc: 0.4942    \n",
      "Epoch 3/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6933 - acc: 0.5002    \n",
      "Epoch 4/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6932 - acc: 0.5012    \n",
      "Epoch 5/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6932 - acc: 0.4955    \n",
      "Epoch 6/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6932 - acc: 0.4946    \n",
      "Epoch 7/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6933 - acc: 0.4925    \n",
      "Epoch 8/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6932 - acc: 0.5006    \n",
      "Epoch 9/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6933 - acc: 0.4916    \n",
      "Epoch 10/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6932 - acc: 0.4877    \n",
      "4055/4080 [============================>.] - ETA: 0sEpoch 1/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.7008 - acc: 0.4953    \n",
      "Epoch 2/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6933 - acc: 0.4959    \n",
      "Epoch 3/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6933 - acc: 0.4972    \n",
      "Epoch 4/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6932 - acc: 0.4958    \n",
      "Epoch 5/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6932 - acc: 0.5009    \n",
      "Epoch 6/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6932 - acc: 0.4960    \n",
      "Epoch 7/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6932 - acc: 0.5024    \n",
      "Epoch 8/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6932 - acc: 0.4982    \n",
      "Epoch 9/10\n",
      "16320/16320 [==============================] - 16s - loss: 0.6932 - acc: 0.4944    \n",
      "Epoch 10/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6932 - acc: 0.4955    \n",
      "3925/4080 [===========================>..] - ETA: 0sResults: 50.01% (0.01%)\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Evaluate model A\n",
    "seed = 7\n",
    "estimator = KerasClassifier(build_fn=create_model_A, epochs=10, batch_size=5, verbose=1)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X_10200_10200_combo_sample_number_50_train, Y_10200_10200_combo_sample_number_50_train, cv=kfold)\n",
    "\n",
    "print(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
