{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras as kr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "from functools import reduce\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D, Input, MaxPooling2D, AveragePooling2D\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUMBER_SIZE = 15\n",
    "THRESHOLD_COUNT = 5\n",
    "\n",
    "BATCH_NUMBER_COUNT_50 = 50\n",
    "SINGLE_SAMPLE_SIZE_50 = BATCH_NUMBER_COUNT_50 * NUMBER_SIZE\n",
    "\n",
    "CLASSES = { \"none\": 0, \"attack\": 1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_single_sample_from_array(numbers):\n",
    "  # Treat the list of phone numbers as a single huge list digits\n",
    "  # pretty much like greyscale images, etc.\n",
    "  digit_str = ''.join([str(i) for i in numbers])\n",
    "  # Now create an image-like array\n",
    "  return list(digit_str)\n",
    "\n",
    "def create_single_sample_from_dataframe(dataframe_row):\n",
    "  return create_single_sample_from_array(np.array(dataframe_row))\n",
    "\n",
    "def combine_dfs(dfs, outcome_values=None, shuffle=True):\n",
    "  # Combine data with sequence with outcome\n",
    "  df_list = []\n",
    "  for idx, df in enumerate(dfs):\n",
    "    data_arr = []\n",
    "    for index, series in df.iterrows():\n",
    "      series_value = [v for _, v in series.iteritems()]\n",
    "      combined_series = create_single_sample_from_array(series_value)\n",
    "      data_arr.append(combined_series)\n",
    "    \n",
    "    df_arr_row_count = len(data_arr)\n",
    "    \n",
    "    outcome_value = outcome_values[idx] if outcome_values else idx\n",
    "    data_outcome_tuples = zip(data_arr, [outcome_value] * df_arr_row_count)\n",
    "\n",
    "    # DEBUG\n",
    "    print(\"outcome_value:%s of all possible outcome_values:%s\" % (outcome_value, outcome_values))\n",
    "\n",
    "    df_list += data_outcome_tuples   \n",
    "    \n",
    "  # Merge data with sequence and no sequence\n",
    "  dataframe = pd.DataFrame(df_list, columns=[\"sample\",\"outcome\"])\n",
    "  # Shuffle (or rather randomly select samples) but 1.0 means all\n",
    "  df_random = dataframe.sample(frac=1) if shuffle else dataframe\n",
    "  return df_random\n",
    "\n",
    "def extract_sample_and_outcome(df, sample_col_name='sample', outcome_col_name='outcome'):\n",
    "  X = [i for i in df[sample_col_name]]\n",
    "  Y = [i for i in df[outcome_col_name]]\n",
    "  return (X, Y)\n",
    "\n",
    "def load_data(filename, expected_shape_tuple):\n",
    "  data = pd.read_csv(filename, header=None)\n",
    "  print(\"data file:%s, data.shape (should be %s): %s\" % (filename, expected_shape_tuple, data.shape))\n",
    "  if data.shape != expected_shape_tuple:\n",
    "    raise ValueError(\"data.shape:%s does not match excpected shape:%s\" % (data.shape, expected_shape_tuple))\n",
    "  return data\n",
    "    \n",
    "def prepare_data(filenames, expected_shape_tuples, outcome_values=None, shuffle=True):\n",
    "  dfs = []\n",
    "  for idx, filename in enumerate(filenames):\n",
    "    dfs.append(load_data(filename, expected_shape_tuples[idx]))\n",
    "\n",
    "  df_all = combine_dfs(dfs, outcome_values, shuffle)\n",
    "  if (df_all.shape[0] != reduce((lambda m, i: m + i.shape[0]), dfs,0)):\n",
    "    raise ValueError(\"There is a problem with combine_dfs. df_all.shape:%s does not match the sum of df_seq:%s and df_no_seq:%s\" % (df_all.shape, df_seq.shape, df_no_seq.shape))\n",
    "  return df_all\n",
    "\n",
    "def prepare_train_data(dataframe, single_sample_size):\n",
    "  df_row_count = dataframe.shape[0]\n",
    "  (X, Y) = extract_sample_and_outcome(dataframe)\n",
    "  X_train = np.array(X).reshape(df_row_count, 1, single_sample_size, 1)\n",
    "  Y_train = np.array(Y).reshape(df_row_count)\n",
    "\n",
    "  print(\"type(X):%s, len(X):%d, len(X[0]):%d\" % (type(X), len(X), len(X[0])))\n",
    "  print(\"df_row_count: %d\" % df_row_count)\n",
    "  print(\"single_sample_size_bit:%d\" % single_sample_size)\n",
    "  print(\"X_train.shape:%s, Y_train.shape:%s\" % (X_train.shape, Y_train.shape))\n",
    "  print(\"\\n\")\n",
    "\n",
    "  print(\"type(Y[0]):%s, Y[0]:%s\" % (type(Y[0]), Y[0]))\n",
    "\n",
    "  return (X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data file:data_no_sequence_2040_sample_number_50.csv, data.shape (should be (2040, 50)): (2040, 50)\n",
      "data file:data_sequence_sparse_head_heavy_tail_heavy_ooo_mid_combo_2040_sample_number_50.csv, data.shape (should be (2040, 50)): (2040, 50)\n",
      "outcome_value:0 of all possible outcome_values:[0, 1]\n",
      "outcome_value:1 of all possible outcome_values:[0, 1]\n",
      "data file:data_no_sequence_10200_sample_number_50.csv, data.shape (should be (10200, 50)): (10200, 50)\n",
      "data file:data_sequence_sparse_head_heavy_tail_heavy_ooo_mid_combo_10200_sample_number_50.csv, data.shape (should be (10200, 50)): (10200, 50)\n",
      "outcome_value:0 of all possible outcome_values:[0, 1]\n",
      "outcome_value:1 of all possible outcome_values:[0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Load data from files into dataframe\n",
    "\n",
    "df_random_2040_2040_combo_sample_number_50 = prepare_data(['data_no_sequence_2040_sample_number_50.csv','data_sequence_sparse_head_heavy_tail_heavy_ooo_mid_combo_2040_sample_number_50.csv'], [(2040,50), (2040,50)], [CLASSES[\"none\"],CLASSES[\"attack\"]]) \n",
    "\n",
    "df_random_10200_10200_combo_sample_number_50 = prepare_data(['data_no_sequence_10200_sample_number_50.csv','data_sequence_sparse_head_heavy_tail_heavy_ooo_mid_combo_10200_sample_number_50.csv'], [(10200,50), (10200,50)], [CLASSES[\"none\"],CLASSES[\"attack\"]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>[9, 5, 1, 0, 0, 0, 6, 6, 6, 8, 9, 6, 3, 3, 2, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3208</th>\n",
       "      <td>[4, 8, 3, 7, 3, 7, 8, 4, 8, 1, 6, 7, 4, 4, 5, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[9, 2, 8, 5, 1, 4, 0, 2, 3, 9, 6, 9, 1, 9, 8, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>[4, 4, 4, 8, 8, 4, 5, 1, 5, 6, 3, 1, 4, 7, 5, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>[2, 8, 1, 6, 7, 1, 7, 9, 6, 4, 8, 2, 5, 6, 0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sample  outcome\n",
       "522   [9, 5, 1, 0, 0, 0, 6, 6, 6, 8, 9, 6, 3, 3, 2, ...        0\n",
       "3208  [4, 8, 3, 7, 3, 7, 8, 4, 8, 1, 6, 7, 4, 4, 5, ...        1\n",
       "122   [9, 2, 8, 5, 1, 4, 0, 2, 3, 9, 6, 9, 1, 9, 8, ...        0\n",
       "319   [4, 4, 4, 8, 8, 4, 5, 1, 5, 6, 3, 1, 4, 7, 5, ...        0\n",
       "72    [2, 8, 1, 6, 7, 1, 7, 9, 6, 4, 8, 2, 5, 6, 0, ...        0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #1\n",
    "\n",
    "# There should be 'sample' and 'outcome' columns\n",
    "# 'sample' contains all phone numbers in period broken down into individual digits\n",
    "# and combined\n",
    "# 'outcome' is 0 or 1 for binary\n",
    "# Also there should be mixed 0 and 1 since the samples have been randomize\n",
    "df_random_2040_2040_combo_sample_number_50.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2040"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #2\n",
    "\n",
    "# 50% or 2040 samples should be 1\n",
    "np.count_nonzero(df_random_2040_2040_combo_sample_number_50['outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "750"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK #3\n",
    "\n",
    "# The 'sample' length should be same as SINGLE_SAMPLE_SIZE_50 = BATCH_NUMBER_COUNT_50 * NUMBER_SIZE\n",
    "len(df_random_2040_2040_combo_sample_number_50['sample'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(X):<class 'list'>, len(X):4080, len(X[0]):750\n",
      "df_row_count: 4080\n",
      "single_sample_size_bit:750\n",
      "X_train.shape:(4080, 1, 750, 1), Y_train.shape:(4080,)\n",
      "\n",
      "\n",
      "type(Y[0]):<class 'numpy.int64'>, Y[0]:0\n",
      "type(X):<class 'list'>, len(X):20400, len(X[0]):750\n",
      "df_row_count: 20400\n",
      "single_sample_size_bit:750\n",
      "X_train.shape:(20400, 1, 750, 1), Y_train.shape:(20400,)\n",
      "\n",
      "\n",
      "type(Y[0]):<class 'numpy.int64'>, Y[0]:1\n"
     ]
    }
   ],
   "source": [
    "# Split out dataframe containing both samples and outcome\n",
    "\n",
    "(X_2040_2040_combo_sample_number_50_train, Y_2040_2040_combo_sample_number_50_train) = prepare_train_data(df_random_2040_2040_combo_sample_number_50, SINGLE_SAMPLE_SIZE_50)\n",
    "\n",
    "(X_10200_10200_combo_sample_number_50_train, Y_10200_10200_combo_sample_number_50_train) = prepare_train_data(df_random_10200_10200_combo_sample_number_50, SINGLE_SAMPLE_SIZE_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4080, 1, 750, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For CNN, the dimensions are number of samples, height, width, channel/feature maps\n",
    "# Number of samples: sum of 'attack' and 'no attack' samples\n",
    "# Height: 1\n",
    "# Width: SINGLE_SAMPLE_SIZE_50\n",
    "# Feature maps/Channels: 1\n",
    "X_2040_2040_combo_sample_number_50_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4080,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_2040_2040_combo_sample_number_50_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Various multi-class CNN models tested\n",
    "\n",
    "def create_cnn_model_G_50():\n",
    "  # Create model\n",
    "  model = Sequential()\n",
    "  # Max number of digits are different in number yet still can be within threshold\n",
    "  digit_diff_size = 2\n",
    "  digit_same_size = NUMBER_SIZE - digit_diff_size\n",
    "  # If the seq is in the middle then parts that are same is split in two\n",
    "  smallest_digit_same_size = math.floor(digit_same_size/2)\n",
    "  # add model layers\n",
    "  number_digit_diff_size = math.floor(NUMBER_SIZE/digit_diff_size)\n",
    "  model.add(Conv2D(250, kernel_size=(1,smallest_digit_same_size), strides=(1,1), activation='relu', input_shape=(1,SINGLE_SAMPLE_SIZE_50,1)))\n",
    "  model.add(Conv2D(50, kernel_size=(1,NUMBER_SIZE), strides=(1,NUMBER_SIZE), activation='relu'))\n",
    "  #model.add(Conv2D(THRESHOLD_COUNT, kernel_size=(1,NUMBER_SIZE_BIT), activation='relu'))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(BATCH_NUMBER_COUNT_50, activation=\"relu\"))\n",
    "  model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_cnn_model_G_50_coarse():\n",
    "  # Create model\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(5, kernel_size=(1,NUMBER_SIZE), strides=(1,NUMBER_SIZE), activation='relu', input_shape=(1,SINGLE_SAMPLE_SIZE_50,1)))\n",
    "  model.add(Conv2D(2, kernel_size=(1,1), strides=(1,1), activation='relu'))\n",
    "  #model.add(Conv2D(THRESHOLD_COUNT, kernel_size=(1,NUMBER_SIZE_BIT), activation='relu'))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(BATCH_NUMBER_COUNT_50, activation=\"relu\"))\n",
    "  model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_cnn_model_G_50_mod():\n",
    "  # Create model\n",
    "  model = Sequential()\n",
    "  # Max number of bits are different in number yet still can be within threshold\n",
    "  bit_diff_size = BIT_PER_DIGIT*2\n",
    "  bit_same_size = NUMBER_SIZE_BIT - bit_diff_size\n",
    "  # If the seq is in the middle then parts that are same is split in two\n",
    "  smallest_bit_same_size = int(math.floor(bit_same_size/2) / BIT_PER_DIGIT)\n",
    "  # add model layers\n",
    "  number_bit_diff_size = math.floor(NUMBER_SIZE_BIT/bit_diff_size)\n",
    "  model.add(Conv2D(150, kernel_size=(1,smallest_bit_same_size), strides=(1,BIT_PER_DIGIT), activation='relu', input_shape=(1,SINGLE_SAMPLE_SIZE_BIT_50,1)))\n",
    "  model.add(Conv2D(5, kernel_size=(1,NUMBER_SIZE_BIT ), strides=(1,NUMBER_SIZE_BIT), activation='relu'))\n",
    "  #model.add(Conv2D(THRESHOLD_COUNT, kernel_size=(1,NUMBER_SIZE_BIT), activation='relu'))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(10 - THRESHOLD_COUNT, activation=\"relu\"))\n",
    "  model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_cnn_model_G_50_maxpool():\n",
    "  # Create model\n",
    "  model = Sequential()\n",
    "  # Max number of bits are different in number yet still can be within threshold\n",
    "  # bit_diff_size = BIT_PER_DIGIT*2\n",
    "  bit_same_size = NUMBER_SIZE_BIT - bit_diff_size\n",
    "  # If the seq is in the middle then parts that are same is split in two\n",
    "  smallest_bit_same_size = math.floor(bit_same_size/2)\n",
    "  # add model layers\n",
    "  number_bit_diff_size = math.floor(NUMBER_SIZE_BIT/bit_diff_size)\n",
    "  model.add(Conv2D(150, kernel_size=(1,smallest_bit_same_size), strides=(1,BIT_PER_DIGIT), activation='relu', input_shape=(1,SINGLE_SAMPLE_SIZE_BIT_50,1)))\n",
    "  model.add(MaxPooling2D(pool_size=(1,smallest_bit_same_size), strides=(1,math.floor(smallest_bit_same_size/BIT_PER_DIGIT))))\n",
    "  model.add(Conv2D(5, kernel_size=(1,1), strides=(1,1), activation='relu'))\n",
    "  #model.add(Conv2D(5, kernel_size=(1,NUMBER_SIZE_BIT ), strides=(1,NUMBER_SIZE_BIT), activation='relu'))\n",
    "  #model.add(Conv2D(THRESHOLD_COUNT, kernel_size=(1,NUMBER_SIZE_BIT), activation='relu'))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(10 - THRESHOLD_COUNT, activation=\"relu\"))\n",
    "  model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_cnn_model_G_50_avepool():\n",
    "  # Create model\n",
    "  model = Sequential()\n",
    "  # Max number of bits are different in number yet still can be within threshold\n",
    "  digit_diff_size = 2\n",
    "  digit_same_size = NUMBER_SIZE - digit_diff_size\n",
    "  # If the seq is in the middle then parts that are same is split in two\n",
    "  smallest_digit_same_size = math.floor(digit_same_size/2)\n",
    "  # add model layers\n",
    "  number_digit_diff_size = math.floor(NUMBER_SIZE/digit_diff_size)\n",
    "  model.add(Conv2D(150, kernel_size=(1,smallest_digit_same_size), strides=(1,1), activation='relu', input_shape=(1,SINGLE_SAMPLE_SIZE_50,1)))\n",
    "  model.add(AveragePooling2D(pool_size=(1,NUMBER_SIZE), strides=(1,NUMBER_SIZE)))\n",
    "  # model.add(Conv2D(50, kernel_size=(1,1), strides=(1,1), activation='relu'))\n",
    "  #model.add(Conv2D(5, kernel_size=(1,NUMBER_SIZE_BIT ), strides=(1,NUMBER_SIZE_BIT), activation='relu'))\n",
    "  #model.add(Conv2D(THRESHOLD_COUNT, kernel_size=(1,NUMBER_SIZE_BIT), activation='relu'))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(BATCH_NUMBER_COUNT_50, activation=\"relu\"))\n",
    "  model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_cnn_model_G_50_coarse_3():\n",
    "  # Create model\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(20, kernel_size=(1,NUMBER_SIZE), strides=(1,NUMBER_SIZE), activation='relu', input_shape=(1,SINGLE_SAMPLE_SIZE_50,1)))\n",
    "  model.add(MaxPooling2D(pool_size=(1,1), strides=(1,1)))\n",
    "  model.add(Conv2D(10, kernel_size=(1,1), strides=(1,1), activation='relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(1,1), strides=(1,1)))\n",
    "  #model.add(Conv2D(THRESHOLD_COUNT, kernel_size=(1,NUMBER_SIZE_BIT), activation='relu'))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(BATCH_NUMBER_COUNT_50, activation=\"relu\"))\n",
    "  model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16320 samples, validate on 4080 samples\n",
      "Epoch 1/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.6936 - acc: 0.5060 - val_loss: 0.6938 - val_acc: 0.4929\n",
      "Epoch 2/50\n",
      "16320/16320 [==============================] - 9s - loss: 0.6821 - acc: 0.5428 - val_loss: 0.6692 - val_acc: 0.5777\n",
      "Epoch 3/50\n",
      "16320/16320 [==============================] - 9s - loss: 0.6482 - acc: 0.6046 - val_loss: 0.6623 - val_acc: 0.5779\n",
      "Epoch 4/50\n",
      "16320/16320 [==============================] - 9s - loss: 0.6179 - acc: 0.6482 - val_loss: 0.6529 - val_acc: 0.6047\n",
      "Epoch 5/50\n",
      "16320/16320 [==============================] - 9s - loss: 0.5853 - acc: 0.6822 - val_loss: 0.6479 - val_acc: 0.6203\n",
      "Epoch 6/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.5480 - acc: 0.7153 - val_loss: 0.6673 - val_acc: 0.6159\n",
      "Epoch 7/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.4999 - acc: 0.7500 - val_loss: 0.7121 - val_acc: 0.6076\n",
      "Epoch 8/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.4446 - acc: 0.7894 - val_loss: 0.7545 - val_acc: 0.6007\n",
      "Epoch 9/50\n",
      "16320/16320 [==============================] - 9s - loss: 0.3883 - acc: 0.8208 - val_loss: 0.8472 - val_acc: 0.5990\n",
      "Epoch 10/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.3263 - acc: 0.8583 - val_loss: 0.9815 - val_acc: 0.5914\n",
      "Epoch 11/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.2672 - acc: 0.8856 - val_loss: 1.1382 - val_acc: 0.5875\n",
      "Epoch 12/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.2149 - acc: 0.9113 - val_loss: 1.3110 - val_acc: 0.5836\n",
      "Epoch 13/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.1688 - acc: 0.9341 - val_loss: 1.5194 - val_acc: 0.5846\n",
      "Epoch 14/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.1369 - acc: 0.9466 - val_loss: 1.6858 - val_acc: 0.5843\n",
      "Epoch 15/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.1106 - acc: 0.9605 - val_loss: 1.8860 - val_acc: 0.5833\n",
      "Epoch 16/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.0955 - acc: 0.9645 - val_loss: 2.0925 - val_acc: 0.5819\n",
      "Epoch 17/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.0866 - acc: 0.9676 - val_loss: 2.1197 - val_acc: 0.5821\n",
      "Epoch 18/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.0752 - acc: 0.9727 - val_loss: 2.2971 - val_acc: 0.5750\n",
      "Epoch 19/50\n",
      "16320/16320 [==============================] - 11s - loss: 0.0619 - acc: 0.9768 - val_loss: 2.4588 - val_acc: 0.5887\n",
      "Epoch 20/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.0634 - acc: 0.9760 - val_loss: 2.7279 - val_acc: 0.5873\n",
      "Epoch 21/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.0635 - acc: 0.9760 - val_loss: 2.7799 - val_acc: 0.5806\n",
      "Epoch 22/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.0616 - acc: 0.9775 - val_loss: 2.8447 - val_acc: 0.5819\n",
      "Epoch 23/50\n",
      "16320/16320 [==============================] - 9s - loss: 0.0561 - acc: 0.9795 - val_loss: 2.8537 - val_acc: 0.5792\n",
      "Epoch 24/50\n",
      "16320/16320 [==============================] - 9s - loss: 0.0513 - acc: 0.9820 - val_loss: 3.0196 - val_acc: 0.5733\n",
      "Epoch 25/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.0470 - acc: 0.9831 - val_loss: 3.2584 - val_acc: 0.5828\n",
      "Epoch 26/50\n",
      "16320/16320 [==============================] - 9s - loss: 0.0548 - acc: 0.9814 - val_loss: 2.9413 - val_acc: 0.5716\n",
      "Epoch 27/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.0508 - acc: 0.9807 - val_loss: 2.9427 - val_acc: 0.5775\n",
      "Epoch 28/50\n",
      "16320/16320 [==============================] - 9s - loss: 0.0411 - acc: 0.9843 - val_loss: 3.3852 - val_acc: 0.5743\n",
      "Epoch 29/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.0497 - acc: 0.9839 - val_loss: 3.1431 - val_acc: 0.5743\n",
      "Epoch 30/50\n",
      "16320/16320 [==============================] - 9s - loss: 0.0393 - acc: 0.9850 - val_loss: 3.0901 - val_acc: 0.5740\n",
      "Epoch 31/50\n",
      "16320/16320 [==============================] - 9s - loss: 0.0515 - acc: 0.9821 - val_loss: 3.2805 - val_acc: 0.5797\n",
      "Epoch 32/50\n",
      "16320/16320 [==============================] - 9s - loss: 0.0352 - acc: 0.9879 - val_loss: 3.3491 - val_acc: 0.5733\n",
      "Epoch 33/50\n",
      "16320/16320 [==============================] - 9s - loss: 0.0438 - acc: 0.9852 - val_loss: 3.3482 - val_acc: 0.5745\n",
      "Epoch 34/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.0354 - acc: 0.9873 - val_loss: 3.4093 - val_acc: 0.5801\n",
      "Epoch 35/50\n",
      "16320/16320 [==============================] - 9s - loss: 0.0367 - acc: 0.9860 - val_loss: 3.4669 - val_acc: 0.5735\n",
      "Epoch 36/50\n",
      "16320/16320 [==============================] - 9s - loss: 0.0405 - acc: 0.9858 - val_loss: 3.3952 - val_acc: 0.5743\n",
      "Epoch 37/50\n",
      "16320/16320 [==============================] - 9s - loss: 0.0354 - acc: 0.9879 - val_loss: 3.4807 - val_acc: 0.5762\n",
      "Epoch 38/50\n",
      "16320/16320 [==============================] - 9s - loss: 0.0368 - acc: 0.9869 - val_loss: 3.5908 - val_acc: 0.5645\n",
      "Epoch 39/50\n",
      "16320/16320 [==============================] - 9s - loss: 0.0372 - acc: 0.9876 - val_loss: 3.6987 - val_acc: 0.5740\n",
      "Epoch 40/50\n",
      "16320/16320 [==============================] - 9s - loss: 0.0390 - acc: 0.9870 - val_loss: 3.4990 - val_acc: 0.5723\n",
      "Epoch 41/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.0317 - acc: 0.9882 - val_loss: 3.5918 - val_acc: 0.5777\n",
      "Epoch 42/50\n",
      "16320/16320 [==============================] - 9s - loss: 0.0318 - acc: 0.9900 - val_loss: 3.6501 - val_acc: 0.5760\n",
      "Epoch 43/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.0336 - acc: 0.9877 - val_loss: 3.6445 - val_acc: 0.5765\n",
      "Epoch 44/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.0324 - acc: 0.9888 - val_loss: 3.6555 - val_acc: 0.5809\n",
      "Epoch 45/50\n",
      "16320/16320 [==============================] - 9s - loss: 0.0368 - acc: 0.9870 - val_loss: 3.6730 - val_acc: 0.5799\n",
      "Epoch 46/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.0250 - acc: 0.9919 - val_loss: 3.7012 - val_acc: 0.5814\n",
      "Epoch 47/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.0333 - acc: 0.9882 - val_loss: 3.6853 - val_acc: 0.5789\n",
      "Epoch 48/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.0318 - acc: 0.9887 - val_loss: 3.7444 - val_acc: 0.5730\n",
      "Epoch 49/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.0243 - acc: 0.9917 - val_loss: 3.7529 - val_acc: 0.5787\n",
      "Epoch 50/50\n",
      "16320/16320 [==============================] - 10s - loss: 0.0427 - acc: 0.9853 - val_loss: 3.7247 - val_acc: 0.5770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x116563f60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For multi-class this is best so we use it here in binary class CNN: create_cnn_model_G_50_coarse_3\n",
    "\n",
    "cnn_model_G_50_coarse_3 = create_cnn_model_G_50_coarse_3()\n",
    "\n",
    "cnn_model_G_50_coarse_3.fit(X_10200_10200_combo_sample_number_50_train, Y_10200_10200_combo_sample_number_50_train, validation_split=0.2, batch_size=10, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3936/4080 [===========================>..] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6056372549019607"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_arr = cnn_model_G_50_coarse_3.predict_classes(X_2040_2040_combo_sample_number_50_train)\n",
    "# Each element in 'a' will contain 1/True if prediction matches expected outcome\n",
    "a = (predict_arr == np.array([[r] for r in Y_2040_2040_combo_sample_number_50_train]))\n",
    "# % of correct predictions\n",
    "np.count_nonzero(a)/np.size(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6932 - acc: 0.5117    \n",
      "Epoch 2/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6760 - acc: 0.5594    \n",
      "Epoch 3/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6563 - acc: 0.5938    \n",
      "Epoch 4/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6390 - acc: 0.6176    \n",
      "Epoch 5/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6188 - acc: 0.6453    \n",
      "Epoch 6/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.5972 - acc: 0.6653    \n",
      "Epoch 7/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.5688 - acc: 0.6939    \n",
      "Epoch 8/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.5351 - acc: 0.7195    \n",
      "Epoch 9/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.4995 - acc: 0.7483    \n",
      "Epoch 10/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.4504 - acc: 0.7803    \n",
      "4050/4080 [============================>.] - ETA: 0sEpoch 1/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6938 - acc: 0.5021    \n",
      "Epoch 2/10\n",
      "16320/16320 [==============================] - 16s - loss: 0.6818 - acc: 0.5426    \n",
      "Epoch 3/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6516 - acc: 0.6010    \n",
      "Epoch 4/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6331 - acc: 0.6292    \n",
      "Epoch 5/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6166 - acc: 0.6468    \n",
      "Epoch 6/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6014 - acc: 0.6676    \n",
      "Epoch 7/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.5819 - acc: 0.6853    \n",
      "Epoch 8/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.5561 - acc: 0.7081    \n",
      "Epoch 9/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.5244 - acc: 0.7336    \n",
      "Epoch 10/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.4892 - acc: 0.7562    \n",
      "3945/4080 [============================>.] - ETA: 0sEpoch 1/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6947 - acc: 0.5052    \n",
      "Epoch 2/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6760 - acc: 0.5537    \n",
      "Epoch 3/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6424 - acc: 0.6191    \n",
      "Epoch 4/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6171 - acc: 0.6529    \n",
      "Epoch 5/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.5953 - acc: 0.6770    \n",
      "Epoch 6/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.5732 - acc: 0.6945    \n",
      "Epoch 7/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.5469 - acc: 0.7123    \n",
      "Epoch 8/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.5156 - acc: 0.7368    \n",
      "Epoch 9/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.4827 - acc: 0.7619    \n",
      "Epoch 10/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.4514 - acc: 0.7774    \n",
      "4055/4080 [============================>.] - ETA: 0sEpoch 1/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6933 - acc: 0.5034    \n",
      "Epoch 2/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6700 - acc: 0.5692    \n",
      "Epoch 3/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6444 - acc: 0.6112    \n",
      "Epoch 4/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6241 - acc: 0.6383    \n",
      "Epoch 5/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.6071 - acc: 0.6598    \n",
      "Epoch 6/10\n",
      "16320/16320 [==============================] - 15s - loss: 0.5888 - acc: 0.6765    \n",
      "Epoch 7/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.5652 - acc: 0.6983    \n",
      "Epoch 8/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.5383 - acc: 0.7232    \n",
      "Epoch 9/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.5095 - acc: 0.7428    \n",
      "Epoch 10/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.4719 - acc: 0.7667    \n",
      "3970/4080 [============================>.] - ETA: 0sEpoch 1/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6940 - acc: 0.4977    \n",
      "Epoch 2/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6886 - acc: 0.5290    \n",
      "Epoch 3/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6560 - acc: 0.5964    \n",
      "Epoch 4/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6318 - acc: 0.6346    \n",
      "Epoch 5/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6137 - acc: 0.6571    \n",
      "Epoch 6/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.6007 - acc: 0.6704    \n",
      "Epoch 7/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.5902 - acc: 0.6831    \n",
      "Epoch 8/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.5782 - acc: 0.6905    \n",
      "Epoch 9/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.5687 - acc: 0.7015    \n",
      "Epoch 10/10\n",
      "16320/16320 [==============================] - 14s - loss: 0.5551 - acc: 0.7129    \n",
      "4015/4080 [============================>.] - ETA: 0sResults: 61.60% (1.78%)\n"
     ]
    }
   ],
   "source": [
    "# Alternatively you can evaluate using StratifiedKFold\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Evaluate baseline model\n",
    "seed = 7\n",
    "estimator = KerasClassifier(build_fn=create_cnn_model_G_50_coarse_3, epochs=10, batch_size=5, verbose=1)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X_10200_10200_combo_sample_number_50_train, Y_10200_10200_combo_sample_number_50_train, cv=kfold)\n",
    "\n",
    "print(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
